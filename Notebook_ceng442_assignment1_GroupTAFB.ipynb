{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eiziiaizii1/ceng442-assignment1-GroupTAFB/blob/main/Notebook_ceng442_assignment1_GroupTAFB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYv14tsAwLov"
      },
      "source": [
        "# CENG442 Assignment 1 - Azerbaijani Text Preprocessing & Word Embeddings\n",
        "\n",
        "**Group Members:**\n",
        "* Talha Ubeydullah Gamga | 20050111078\n",
        "* Aziz Önder | 22050141021\n",
        "* Muhammed Fatih Asan | 23050151026\n",
        "* Buğra Bildiren | 20050111022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhKFLV_wLox"
      },
      "source": [
        "## Step 1: Setup and Imports\n",
        "\n",
        "In this step, we import all necessary libraries for data processing and text cleaning, including standard libraries like `pandas`, `re` (regex), and `ftfy` (for text normalization).\n",
        "\n",
        "We also import the custom utility functions (e.g., domain detection, emoji/negation handling) from the `ozel_temizlik.py` script.\n",
        "\n",
        "Finally, we define and create the `OUTPUT_DIR` (`clean_data/`) where our processed Excel files will be saved."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL INITIALLY, IF YOU ARE RUNNING IN COLAB\n",
        "!git clone https://github.com/eiziiaizii1/ceng442-assignment1-GroupTAFB.git\n",
        "%cd ceng442-assignment1-GroupTAFB\n",
        "!pip install pandas gensim openpyxl regex ftfy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMK10BjM0aA",
        "outputId": "c4537ab3-c83c-49c0-cf44-dfd039719875"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ceng442-assignment1-GroupTAFB'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 82 (delta 39), reused 52 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (82/82), 15.72 MiB | 47.78 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/ceng442-assignment1-GroupTAFB\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, gensim\n",
            "Successfully installed ftfy-6.3.1 gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R6zkzmgwLoy",
        "outputId": "8bf2df32-a47b-4954-afde-28e3f533a56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n",
            "Utility functions from 'ozel_temizlik.py' imported.\n",
            "Output directory 'clean_data' is ready.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import ftfy\n",
        "\n",
        "# --- Import Custom Utility Script ---\n",
        "# This script contains helper functions for domain detection,\n",
        "# negation, emoji mapping, and other specific cleaning tasks.\n",
        "import ozel_temizlik\n",
        "\n",
        "# --- Setup Output Directory ---\n",
        "OUTPUT_DIR = \"clean_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n",
        "print(f\"Utility functions from 'ozel_temizlik.py' imported.\")\n",
        "print(f\"Output directory '{OUTPUT_DIR}' is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRZ8bJyCwLo0"
      },
      "source": [
        "## Step 2: Define Core Helper Functions\n",
        "\n",
        "In this step, we define the core helper functions required by the main processing pipeline. These functions are responsible for:\n",
        "\n",
        "1.  **`map_sentiment_value`**: Standardizing the various sentiment labels (e.g., \"positive\", 1, 0.0) from the 5 datasets into a single numeric float format (0.0, 0.5, 1.0).\n",
        "2.  **`lower_az`**: Handling the specific lowercase conversion for Azerbaijani characters (e.g., 'İ' -> 'i', 'I' -> 'ı').\n",
        "3.  **`basic_regex_clean`**: Applying the fundamental, non-domain-specific cleaning rules (like removing HTML, normalizing URLs, Emails, Numbers) based on the code snippets provided in the PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4QxYQTnwLo0",
        "outputId": "a2b54413-7f2a-4cb0-bd3d-501ac2aa1b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core helper functions (map_sentiment_value, lower_az, basic_regex_clean) defined.\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 2.1: Sentiment Label Standardization\n",
        "# (Maps all labels to 0.0, 0.5, 1.0 as float)\n",
        "# ----------------------------------------------------------------\n",
        "def map_sentiment_value(label):\n",
        "    \"\"\"\n",
        "    Converts various sentiment labels (str, int) from different\n",
        "    datasets into a standard float value (0.0, 0.5, or 1.0).\n",
        "    Returns None if the label is unmappable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Handle string labels\n",
        "    if isinstance(label, str):\n",
        "        label_low = label.lower().strip()\n",
        "        if label_low in ['positive', 'pos', '1']:\n",
        "            return 1.0\n",
        "        elif label_low in ['negative', 'neg', '0']:\n",
        "            return 0.0\n",
        "        elif label_low in ['neutral', 'neu', '0.5']:\n",
        "            return 0.5\n",
        "\n",
        "    # Handle integer labels\n",
        "    if isinstance(label, int):\n",
        "        if label == 1:\n",
        "            return 1.0\n",
        "        elif label == 0:\n",
        "            return 0.0\n",
        "\n",
        "    # Handle float labels\n",
        "    try:\n",
        "        f_label = float(label)\n",
        "        if f_label == 1.0: return 1.0\n",
        "        if f_label == 0.0: return 0.0\n",
        "        if f_label == 0.5: return 0.5\n",
        "    except (ValueError, TypeError):\n",
        "        pass\n",
        "\n",
        "    # If no match is found\n",
        "    return None\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 2.2: Azerbaijani-Specific Lowercasing\n",
        "# (PDF Section 5.1.4: 'İ' -> 'i', 'I' -> 'ı')\n",
        "# ----------------------------------------------------------------\n",
        "def lower_az(text):\n",
        "    \"\"\"Applies Azerbaijani-specific lowercase conversion.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return str(text) # Ensure input is string\n",
        "    text = text.replace('İ', 'i').replace('I', 'ı')\n",
        "    return text.lower() # Apply standard lowercasing\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 2.3: Basic Text Normalization (Regex)\n",
        "# (Based on PDF Section 5.1 code snippets)\n",
        "# ----------------------------------------------------------------\n",
        "def basic_regex_clean(text):\n",
        "    \"\"\"\n",
        "    Applies fundamental regex cleaning rules as specified\n",
        "    in the assignment PDF (e.g., HTML, URL, EMAIL, NUM).\n",
        "    \"\"\"\n",
        "\n",
        "    # Fix broken Unicode (e.g., â€™ -> ’) - Recommended by PDF\n",
        "    text = ftfy.fix_text(text)\n",
        "\n",
        "    # Remove HTML tags (PDF Section 5.1.1)\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "    # Normalize URLs (PDF Section 5.1.2)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '<URL>', text)\n",
        "\n",
        "    # Normalize Emails (PDF Section 5.1.2)\n",
        "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
        "\n",
        "    # Normalize @mentions (PDF Section 5.1.2)\n",
        "    text = re.sub(r'@\\w+', '<USER>', text)\n",
        "\n",
        "    # Normalize Phone (simple rule) (PDF Section 5.1.2)\n",
        "    # (Note: PDF has a typo r'(\\+?d... , corrected to \\d)\n",
        "    text = re.sub(r'(\\+?\\d[\\d\\s-]{7,}\\d)', '<PHONE>', text)\n",
        "\n",
        "    # Normalize Numbers (as per PDF Section 5.1.6)\n",
        "    text = re.sub(r'\\b\\d+[\\.,\\d]*\\b', '<NUM>', text)\n",
        "\n",
        "    # Normalize repeating characters (e.g., çooox -> çoxx) (PDF Section 5.1.6)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"Core helper functions (map_sentiment_value, lower_az, basic_regex_clean) defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdOfZLmxwLo1"
      },
      "source": [
        "## Step 3: Define the Main Normalization Pipeline (normalize_text_az)\n",
        "\n",
        "This is the main \"glue\" function for our pipeline. It's responsible for executing all cleaning steps in the correct logical order.\n",
        "\n",
        "It combines the **basic** cleaning functions (defined in Step 2, e.g., `basic_regex_clean`, `lower_az`) with the **advanced/specialized** functions imported from `ozel_temizlik.py` (e.g., `split_hashtags`, `handle_negation`).\n",
        "\n",
        "The main `process_file` function (which we will use in the next step) will call this single function to perform the complete text normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdciXVZOwLo1",
        "outputId": "a3150339-3c8d-4adc-8198-e5d1ed91567e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main pipeline function 'normalize_text_az' defined.\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 3.1: Main Normalization Pipeline Function\n",
        "# (This function is called by the PDF's process_file skeleton)\n",
        "# ----------------------------------------------------------------\n",
        "def normalize_text_az(raw_text, domain):\n",
        "    \"\"\"\n",
        "    Applies the full sequence of cleaning and normalization steps.\n",
        "\n",
        "    This function combines the basic regex cleaning with the\n",
        "    domain-specific and special challenge functions in a logical order.\n",
        "\n",
        "    Args:\n",
        "        raw_text (str): The original, unprocessed text.\n",
        "        domain (str): The detected domain ('news', 'social', etc.).\n",
        "\n",
        "    Returns:\n",
        "        str: The fully cleaned and normalized text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Basic Azeri Lowercasing (from Step 2)\n",
        "    text = lower_az(raw_text)\n",
        "\n",
        "    # Step 2: Basic Regex Cleaning (HTML, URL, NUM, etc.) (from Step 2)\n",
        "    text = basic_regex_clean(text)\n",
        "\n",
        "    # --- Apply special functions from ozel_temizlik.py ---\n",
        "\n",
        "    # Step 3: Split CamelCase hashtags (e.g., #QarabagIsBack -> qarabag is back)\n",
        "    text = ozel_temizlik.split_hashtags(text)\n",
        "\n",
        "    # Step 4: Map Emojis (e.g., :) -> EMO_POS)\n",
        "    text = ozel_temizlik.map_emojis_and_normalize(text)\n",
        "\n",
        "    # Step 5: Deasciify/Slang (e.g., cox -> çox)\n",
        "    text = ozel_temizlik.deasciify_slang(text)\n",
        "\n",
        "    # Step 6: Domain-Specific Normalization (e.g., 50 azn -> <PRICE>)\n",
        "    # (Must run BEFORE negation to avoid tagging <PRICE>_NEG)\n",
        "    text = ozel_temizlik.domain_specific_normalize(text, domain)\n",
        "\n",
        "    # Step 7: Handle Negation (e.g., \"yoxdur\" -> \"yoxdur\")\n",
        "    # (Note: Negation function in ozel_temizlik.py handles _NEG tagging)\n",
        "    text = ozel_temizlik.handle_negation(text)\n",
        "\n",
        "    # Step 8: Final cleanup (remove extra whitespace created during cleaning)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"Main pipeline function 'normalize_text_az' defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tWVOJ1nwLo2"
      },
      "source": [
        "## Step 4: Define Main Processing Function and Dataset List\n",
        "\n",
        "Now we define the final pieces needed to run the entire pipeline:\n",
        "\n",
        "1.  **`datasets_to_process`**: A list of dictionaries defining the 5 raw data files to process.\n",
        "2.  **`process_file`**: The main function skeleton provided in the assignment PDF. This function reads a file, applies all our helper functions (`map_sentiment_value`, `normalize_text_az`, etc.) in the correct order, removes duplicates/empties, and saves the final two-column Excel file to our `OUTPUT_DIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZI3nvOfwLo3",
        "outputId": "289caf78-3ad6-4e2d-f85f-9cd26f3edad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset list and main 'process_file' function defined.\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 4.1: Define the list of datasets to process\n",
        "# (Based on PDF Section 2)\n",
        "# ----------------------------------------------------------------\n",
        "datasets_to_process = [\n",
        "    {\n",
        "        \"in_file\": \"data/labeled-sentiment.xlsx\",\n",
        "        \"text_col\": \"text\",\n",
        "        \"label_col\": \"sentiment\",\n",
        "        \"out_file\": os.path.join(OUTPUT_DIR, \"labeled-sentiment_2col.xlsx\")\n",
        "    },\n",
        "    {\n",
        "        \"in_file\": \"data/test__1_.xlsx\",\n",
        "        \"text_col\": \"text\",\n",
        "        \"label_col\": \"label\",\n",
        "        \"out_file\": os.path.join(OUTPUT_DIR, \"test_1_2col.xlsx\") # Using PDF canonical name\n",
        "    },\n",
        "    {\n",
        "        \"in_file\": \"data/train__3_.xlsx\",\n",
        "        \"text_col\": \"text\",\n",
        "        \"label_col\": \"label\",\n",
        "        \"out_file\": os.path.join(OUTPUT_DIR, \"train_3_2col.xlsx\") # Using PDF canonical name\n",
        "    },\n",
        "    {\n",
        "        \"in_file\": \"data/train-00000-of-00001.xlsx\",\n",
        "        \"text_col\": \"text\",\n",
        "        \"label_col\": \"labels\",\n",
        "        \"out_file\": os.path.join(OUTPUT_DIR, \"train-00000-of-00001_2col.xlsx\")\n",
        "    },\n",
        "    {\n",
        "        \"in_file\": \"data/merged_dataset_CSV__1_.xlsx\",\n",
        "        \"text_col\": \"text\",\n",
        "        \"label_col\": \"labels\",\n",
        "        \"out_file\": os.path.join(OUTPUT_DIR, \"merged_dataset_CSV_1_2col.xlsx\") # Using PDF canonical name\n",
        "    }\n",
        "]\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 4.2: Define the main processing function\n",
        "# (Based on the skeleton from PDF Section 7.1)\n",
        "# ----------------------------------------------------------------\n",
        "def process_file(in_file, text_col, label_col, out_file):\n",
        "    \"\"\"\n",
        "    Reads a raw dataset, applies the full cleaning pipeline,\n",
        "    and saves the required two-column (cleaned_text, sentiment_value)\n",
        "    Excel file.\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing: {in_file}...\")\n",
        "    try:\n",
        "        # 1. Read Data\n",
        "        df = pd.read_excel(in_file)\n",
        "\n",
        "        # 2. Drop rows with missing text or labels (PDF Section 5.1.7)\n",
        "        df.dropna(subset=[text_col, label_col], inplace=True)\n",
        "\n",
        "        # 3. Ensure text column is string\n",
        "        df[text_col] = df[text_col].astype(str)\n",
        "\n",
        "        # 4. Drop duplicate texts (PDF Section 5.1.7)\n",
        "        df.drop_duplicates(subset=[text_col], inplace=True)\n",
        "\n",
        "        # 5. Map sentiment labels (Using our function from Step 2)\n",
        "        df['sentiment_value'] = df[label_col].apply(map_sentiment_value)\n",
        "\n",
        "        # 6. Drop rows where mapping failed (e.g., unmappable labels)\n",
        "        df.dropna(subset=['sentiment_value'], inplace=True)\n",
        "\n",
        "        # 7. Detect domain (Using function from ozel_temizlik.py)\n",
        "        # (This must be done on the *raw text* to catch hashtags, URLs, etc.)\n",
        "        df['domain'] = df[text_col].apply(ozel_temizlik.detect_domain)\n",
        "\n",
        "        # 8. Apply the main normalization pipeline (Using our function from Step 3)\n",
        "        print(\"Applying normalization pipeline...\")\n",
        "        df['cleaned_text'] = df.apply(\n",
        "            lambda row: normalize_text_az(row[text_col], row['domain']),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # 9. Select only the required two columns\n",
        "        final_df = df[['cleaned_text', 'sentiment_value']]\n",
        "\n",
        "        # 10. Save to Excel\n",
        "        final_df.to_excel(out_file, index=False, engine='openpyxl')\n",
        "\n",
        "        print(f\"SUCCESS: Saved {len(final_df)} rows to {out_file}\")\n",
        "        return len(final_df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR processing {in_file}: {e}\")\n",
        "        return 0\n",
        "\n",
        "print(\"Dataset list and main 'process_file' function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_dIKzERwLo3"
      },
      "source": [
        "## Step 5: Execute the Processing Pipeline\n",
        "\n",
        "Now that all helper functions, the main normalization pipeline (`normalize_text_az`), and the main processing function (`process_file`) are defined, we can execute the process.\n",
        "\n",
        "This final step iterates through the `datasets_to_process` list and calls the `process_file` function for each dataset.\n",
        "\n",
        "The output will be 5 separate `.xlsx` files, saved in the `clean_data/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFUHf7FvwLo4",
        "outputId": "e9baec3b-bd91-4660-f3d5-73f85028a7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the processing of 5 datasets...\n",
            "\n",
            "Processing: data/labeled-sentiment.xlsx...\n",
            "Applying normalization pipeline...\n",
            "SUCCESS: Saved 2955 rows to clean_data/labeled-sentiment_2col.xlsx\n",
            "\n",
            "Processing: data/test__1_.xlsx...\n",
            "Applying normalization pipeline...\n",
            "SUCCESS: Saved 4198 rows to clean_data/test_1_2col.xlsx\n",
            "\n",
            "Processing: data/train__3_.xlsx...\n",
            "Applying normalization pipeline...\n",
            "SUCCESS: Saved 19557 rows to clean_data/train_3_2col.xlsx\n",
            "\n",
            "Processing: data/train-00000-of-00001.xlsx...\n",
            "Applying normalization pipeline...\n",
            "SUCCESS: Saved 41756 rows to clean_data/train-00000-of-00001_2col.xlsx\n",
            "\n",
            "Processing: data/merged_dataset_CSV__1_.xlsx...\n",
            "Applying normalization pipeline...\n",
            "SUCCESS: Saved 55662 rows to clean_data/merged_dataset_CSV_1_2col.xlsx\n",
            "\n",
            "==============================\n",
            "ALL PROCESSING COMPLETE.\n",
            "Total rows processed across all files: 124128\n",
            "Please check the 'clean_data' directory for the 5 Excel files.\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 5.1: Run the main processing loop\n",
        "# ----------------------------------------------------------------\n",
        "print(f\"Starting the processing of {len(datasets_to_process)} datasets...\")\n",
        "total_rows_processed = 0\n",
        "\n",
        "for dataset in datasets_to_process:\n",
        "    # Call the main function defined in Step 4\n",
        "    rows = process_file(\n",
        "        in_file=dataset[\"in_file\"],\n",
        "        text_col=dataset[\"text_col\"],\n",
        "        label_col=dataset[\"label_col\"],\n",
        "        out_file=dataset[\"out_file\"]\n",
        "    )\n",
        "    total_rows_processed += rows\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(f\"ALL PROCESSING COMPLETE.\")\n",
        "print(f\"Total rows processed across all files: {total_rows_processed}\")\n",
        "print(f\"Please check the '{OUTPUT_DIR}' directory for the 5 Excel files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Building the domain-tagged corpus_all.txt file\n",
        "\n",
        "This process involves:\n",
        "1.  Looping through the **raw** datasets defined in `datasets_to_process`.\n",
        "2.  Using the `ozel_temizlik.detect_domain` function on the raw text.\n",
        "3.  Using the `normalize_text_az` function to clean the text.\n",
        "4.  Prepending the lowercase domain tag (e.g., `domsocial`) as specified in the PDF.\n",
        "5.  Saving all lines to a single `corpus_all.txt` file ."
      ],
      "metadata": {
        "id": "4WpyGCCL566o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------\n",
        "# 6: Build the domain-tagged corpus_all.txt file\n",
        "# (Based on the skeleton from PDF Section 7.1 )\n",
        "# ----------------------------------------------------------------\n",
        "print(f\"Starting generation of 'corpus_all.txt'...\")\n",
        "\n",
        "# This list is already in memory from Step 4\n",
        "# datasets_to_process = [...]\n",
        "\n",
        "output_corpus_file = \"corpus_all.txt\"\n",
        "total_lines = 0\n",
        "\n",
        "# Open the single output file to write to\n",
        "with open(output_corpus_file, \"w\", encoding=\"utf-8\") as f_out:\n",
        "\n",
        "    # Loop over the *raw* datasets\n",
        "    for dataset in datasets_to_process:\n",
        "        in_file = dataset[\"in_file\"]\n",
        "        text_col = dataset[\"text_col\"]\n",
        "        print(f\"Reading raw data from: {in_file}...\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_excel(in_file)\n",
        "\n",
        "            # Drop rows with missing text\n",
        "            df.dropna(subset=[text_col], inplace=True)\n",
        "\n",
        "            for raw_text in df[text_col].astype(str):\n",
        "                # 1. Detect domain from raw text\n",
        "                # (Using the function from ozel_temizlik.py)\n",
        "                domain = ozel_temizlik.detect_domain(raw_text)\n",
        "\n",
        "                # 2. Normalize text using the main pipeline from Step 3\n",
        "                # This pipeline already lowercases and removes punctuation\n",
        "                cleaned_text = normalize_text_az(raw_text, domain)\n",
        "\n",
        "                # 3. Prepend the domain tag\n",
        "                # e.g., \"domsocial bu çox yaxşı\"\n",
        "                if cleaned_text:\n",
        "                    # Using lowercase 'dom{domain}' format from PDF\n",
        "                    line = f\"dom{domain} {cleaned_text}\"\n",
        "                    f_out.write(line + \"\\n\")\n",
        "                    total_lines += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! ERROR processing {in_file} for corpus: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(f\"CORPUS GENERATION COMPLETE.\")\n",
        "print(f\"Total lines saved to '{output_corpus_file}': {total_lines}\")"
      ],
      "metadata": {
        "id": "zxVUC2kQ5m_5",
        "outputId": "a5b49f74-dda9-4026-9f63-2c4825a7141a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting generation of 'corpus_all.txt'...\n",
            "Reading raw data from: data/labeled-sentiment.xlsx...\n",
            "Reading raw data from: data/test__1_.xlsx...\n",
            "Reading raw data from: data/train__3_.xlsx...\n",
            "Reading raw data from: data/train-00000-of-00001.xlsx...\n",
            "Reading raw data from: data/merged_dataset_CSV__1_.xlsx...\n",
            "\n",
            "==============================\n",
            "CORPUS GENERATION COMPLETE.\n",
            "Total lines saved to 'corpus_all.txt': 124431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Training Word2Vec and FastText embedding Models\n",
        "\n",
        "Following the preprocessing steps, we now have five cleaned Excel files. The next task is to train the **Word2Vec** and **FastText** models as specified in the assignment.\n",
        "\n",
        "The code below performs the following actions:\n",
        "1.  Initializes an empty list called `sentences`.\n",
        "2.  Loops through each of the five `_2col.xlsx` files and reads the `cleaned_text` column using `pandas`.\n",
        "3.  Converts each row of cleaned text into a list of tokens (by splitting on spaces) and adds these lists to the main `sentences` collection.\n",
        "4.  Creates the `embeddings/` directory if it doesn't already exist.\n",
        "5.  Trains a `Word2Vec` model using the `sentences` corpus. Key parameters include `vector_size=300`, `window=5`, `min_count=3`, and `sg=1` (Skip-gram).\n",
        "6.  Trains a `FastText` model using the same corpus and similar parameters, but also includes subword information (`min_n=3`, `max_n=6`).\n",
        "7.  Saves both trained models to the `embeddings/` folder as `word2vec.model` and `fasttext.model`."
      ],
      "metadata": {
        "id": "raDxvkCP87Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "files = [\n",
        "    f\"{OUTPUT_DIR}/labeled-sentiment_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/test_1_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/train_3_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/train-00000-of-00001_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/merged_dataset_CSV_1_2col.xlsx\",\n",
        "]\n",
        "\n",
        "sentences = []\n",
        "for f in files:\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
        "\n",
        "Path(\"embeddings\").mkdir(exist_ok=True)\n",
        "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1,\n",
        "negative=10, epochs=10)\n",
        "w2v.save(\"embeddings/word2vec.model\")\n",
        "ft  = FastText(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1,\n",
        "min_n=3, max_n=6, epochs=10)\n",
        "ft.save(\"embeddings/fasttext.model\")\n",
        "print(\"Saved embeddings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lklVCJJ6xolQ",
        "outputId": "e6b439e0-d49b-4504-daf7-d13adcbbe39a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Model Evaluation: Word2Vec vs. FastText (Quantitative & Qualitative Metrics)\n",
        "\n",
        "Model Evaluation section presents a comparative evaluation of the generated Word2Vec and FastText models. To assess their respective strengths in capturing the semantics of the Azerbaijani corpus, the analysis employs three distinct evaluation metrics.\n",
        "\n",
        "### 1. Lexical Coverage (Quantitative)\n",
        "\n",
        "This metric quantifies the **vocabulary coverage** of each model, measuring the percentage of unique tokens from our cleaned datasets that are found within the model's learned vocabulary.\n",
        "\n",
        "This is a critical test for comparing the two architectures. Word2Vec, being a word-level model, is inherently limited to its training vocabulary and cannot represent **out-of-vocabulary (OOV)** words. In contrast, FastText, which learns vectors for character n-grams (subwords), can construct vectors for *any* word, including neologisms, misspellings, or rare words not encountered during training.\n",
        "\n",
        "### 2. Semantic Similarity (Quantitative)\n",
        "\n",
        "A successful embedding model should capture meaningful **semantic relationships**, placing words with similar meanings close together in the vector space and words with opposite meanings far apart.\n",
        "\n",
        "To quantify this, we measure the average **cosine similarity** for two predefined sets of word pairs:\n",
        "* **Synonym Pairs** (e.g., `yaxşı`, `əla`): We expect a high similarity score (close to 1.0), indicating semantic proximity.\n",
        "* **Antonym Pairs** (e.g., `yaxşı`, `pis`): We expect a low or negative similarity score (close to -1.0 or 0.0), indicating semantic distance.\n",
        "\n",
        "A \"Separation Score\" (calculated as `Synonym Similarity - Antonym Similarity`) is then used to provide a single, robust measure of the model's ability to discriminate between semantic similarity and opposition. A higher separation score is better.\n",
        "\n",
        "### 3. Nearest Neighbors Analysis (Qualitative)\n",
        "\n",
        "Beyond quantitative scores, a **qualitative analysis** of the embedding space is performed by inspecting the **nearest neighbors** for a set of predefined seed words.\n",
        "\n",
        "By examining the top 5 most similar words for a given seed (e.g., `bahalı` or `pis`), we can intuitively assess the quality of the learned representations. This helps us judge whether the model has learned logical contexts (e.g., are the neighbors of \"expensive\" other price-related words?) or if it has merely learned superficial co-occurrence patterns."
      ],
      "metadata": {
        "id": "pZeYLp8x8-ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import re\n",
        "\n",
        "w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
        "ft  = FastText.load(\"embeddings/fasttext.model\")\n",
        "\n",
        "seed_words = [\"yaxşı\",\"pis\",\"çox\",\"bahalı\",\"ucuz\",\"mükəmməl\",\"dəhşət\",\"<PRICE>\",\"<RATING_POS>\"]\n",
        "syn_pairs  = [(\"yaxşı\",\"əla\"), (\"bahalı\",\"qiymətli\"), (\"ucuz\",\"sərfəli\")]\n",
        "ant_pairs  = [(\"yaxşı\",\"pis\"), (\"bahalı\",\"ucuz\")]\n",
        "\n",
        "def lexical_coverage(model, tokens):\n",
        "    vocab = model.wv.key_to_index\n",
        "    return sum(1 for t in tokens if t in vocab) / max(1,len(tokens))\n",
        "\n",
        "files = [\n",
        "    f\"{OUTPUT_DIR}/labeled-sentiment_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/test_1_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/train_3_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/train-00000-of-00001_2col.xlsx\",\n",
        "    f\"{OUTPUT_DIR}/merged_dataset_CSV_1_2col.xlsx\",\n",
        "]\n",
        "\n",
        "def read_tokens(f):\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
        "\n",
        "print(\"== Lexical coverage (per dataset) ==\")\n",
        "for f in files:\n",
        "    toks = read_tokens(f)\n",
        "    cov_w2v = lexical_coverage(w2v, toks)\n",
        "    cov_ftv = lexical_coverage(ft, toks)  # FT still embeds OOV via subwords\n",
        "    print(f\"{f}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos(a,b): return float(dot(a,b)/(norm(a)*norm(b)))\n",
        "\n",
        "def pair_sim(model, pairs):\n",
        "    vals = []\n",
        "    for a,b in pairs:\n",
        "        try: vals.append(model.wv.similarity(a,b))\n",
        "        except KeyError: pass\n",
        "    return sum(vals)/len(vals) if vals else float('nan')\n",
        "\n",
        "syn_w2v = pair_sim(w2v, syn_pairs)\n",
        "syn_ft  = pair_sim(ft,  syn_pairs)\n",
        "ant_w2v = pair_sim(w2v, ant_pairs)\n",
        "ant_ft  = pair_sim(ft,  ant_pairs)\n",
        "\n",
        "print(\"\\n== Similarity (higher better for synonyms; lower better for antonyms) ==\")\n",
        "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
        "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
        "print(f\"Separation (Syn - Ant): W2V={(syn_w2v - ant_w2v):.3f}, FT={(syn_ft - ant_ft):.3f}\")\n",
        "\n",
        "def neighbors(model, word, k=5):\n",
        "  try: return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
        "  except KeyError: return []\n",
        "\n",
        "print(\"\\n== Nearest neighbors (qualitative) ==\")\n",
        "for w in seed_words:\n",
        "  print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
        "  print(f\"  FT  NN for '{w}':\", neighbors(ft,  w))\n",
        "\n",
        "# (Optional) domain drift if you train domain-specific models separately:\n",
        "# drift(word, model_a, model_b) = 1 - cos(vec_a, vec_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKK5bBK080FU",
        "outputId": "09bc72b4-65b8-46cb-e4e0-37ffa1bbeb54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Lexical coverage (per dataset) ==\n",
            "clean_data/labeled-sentiment_2col.xlsx: W2V=0.920, FT(vocab)=0.920\n",
            "clean_data/test_1_2col.xlsx: W2V=0.973, FT(vocab)=0.973\n",
            "clean_data/train_3_2col.xlsx: W2V=0.976, FT(vocab)=0.976\n",
            "clean_data/train-00000-of-00001_2col.xlsx: W2V=0.914, FT(vocab)=0.914\n",
            "clean_data/merged_dataset_CSV_1_2col.xlsx: W2V=0.929, FT(vocab)=0.929\n",
            "\n",
            "== Similarity (higher better for synonyms; lower better for antonyms) ==\n",
            "Synonyms: W2V=0.323, FT=0.469\n",
            "Antonyms: W2V=0.313, FT=0.404\n",
            "Separation (Syn - Ant): W2V=0.010, FT=0.065\n",
            "\n",
            "== Nearest neighbors (qualitative) ==\n",
            "  W2V NN for 'yaxşı': ['iyi', 'yaxşi', 'zor', 'yaxshi', 'olardı']\n",
            "  FT  NN for 'yaxşı': ['yaxşı-yaxşı', 'yaxşı!', 'yaxşıı', 'yaxşı)', 'yaxşıkı']\n",
            "  W2V NN for 'pis': ['<STARS_LOW>', 'pisdir', 'örnək', 'kammunal', 'gündedi']\n",
            "  FT  NN for 'pis': ['pis!', '(pis', 'pis,', 'pis.', 'piis']\n",
            "  W2V NN for 'çox': ['bəyəndim', 'çoox', 'tətbiqidir', 'gözəldir', 'çöx']\n",
            "  FT  NN for 'çox': ['çoxçox', 'çox.çox', '(çox', 'çoxx', 'çoxh']\n",
            "  W2V NN for 'bahalı': [',balıq', 'portretlerinə', 'metallarla', 'şəbəkədi', 'tuflisi,']\n",
            "  FT  NN for 'bahalı': ['bahalıı', 'bahalı,', 'bahalısı', 'bahalıq', 'baharlı']\n",
            "  W2V NN for 'ucuz': ['münasib', 'şeytanbazardan', 'düzəltdirilib', 'qiymete', 'unvanı.']\n",
            "  FT  NN for 'ucuz': ['ucuza', 'ucuz.', 'ucuz,', 'ucuzu', '\"ucuz\"']\n",
            "  W2V NN for 'mükəmməl': ['süjetli', 'vestern', 'bayıldım', 'tamamlayır', 'varlıqdır.yaradilanlarin']\n",
            "  FT  NN for 'mükəmməl': ['mükəmməl!', 'mükəmməl,', 'mükəmməl.', 'mükəmməll', 'mükəmməldi']\n",
            "  W2V NN for 'dəhşət': ['xalçalardan', 'ayranları', 'kanaldır.', 'təsirlidi.', 'ağrıyır.']\n",
            "  FT  NN for 'dəhşət': ['\"dəhşət', 'dəhşət!', 'dəhşət.', 'dəhşətdü', 'dəhşətə']\n",
            "  W2V NN for '<PRICE>': ['internetmiz', 'ödəyirik', 'dolamayın', 'zakazı', 'çıxacam']\n",
            "  FT  NN for '<PRICE>': ['<PRICE>a', 'qr=<PRICE>.', 'vps', '50gb', '3m']\n",
            "  W2V NN for '<RATING_POS>': []\n",
            "  FT  NN for '<RATING_POS>': ['ehali', 'zaryatkali', '33du', 'xali', 'morali']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}