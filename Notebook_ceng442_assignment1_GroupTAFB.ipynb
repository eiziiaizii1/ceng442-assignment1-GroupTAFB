{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eiziiaizii1/ceng442-assignment1-GroupTAFB/blob/main/Notebook_ceng442_assignment1_GroupTAFB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYv14tsAwLov"
      },
      "source": [
        "# CENG442 Assignment 1 - Azerbaijani Text Preprocessing & Word Embeddings\n",
        "\n",
        "**Group Members:**\n",
        "* Talha Ubeydullah Gamga | 20050111078\n",
        "* Aziz Önder | 22050141021\n",
        "* Muhammed Fatih Asan | 23050151026\n",
        "* Buğra Bildiren | 20050111022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhKFLV_wLox"
      },
      "source": [
        "This notebook contains a full pipeline for cleaning, normalizing, and preparing Azerbaijani text data for machine learning, with a special focus on sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL INITIALLY, IF YOU ARE RUNNING IN COLAB\n",
        "!git clone https://github.com/eiziiaizii1/ceng442-assignment1-GroupTAFB.git\n",
        "%cd ceng442-assignment1-GroupTAFB\n",
        "!pip install pandas gensim openpyxl regex ftfy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMK10BjM0aA",
        "outputId": "ff40762f-d14f-4323-d120-a50bef285c5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ceng442-assignment1-GroupTAFB'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 91 (delta 46), reused 50 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (91/91), 15.73 MiB | 38.45 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/ceng442-assignment1-GroupTAFB\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, gensim\n",
            "Successfully installed ftfy-6.3.1 gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0R6zkzmgwLoy"
      },
      "outputs": [],
      "source": [
        "import re, html, unicodedata\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import 'ftfy' for fixing text encoding\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "# If 'ftfy' is not installed, create a dummy function\n",
        "except Exception:\n",
        "    def fix_text(s): return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRZ8bJyCwLo0"
      },
      "source": [
        "## Core Normalization Helpers\n",
        "\n",
        "First, we define a function for language-specific lowercasing, as Azerbaijani has unique 'i' and 'I' characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I4QxYQTnwLo0"
      },
      "outputs": [],
      "source": [
        "# Azerbaijani-aware lowercase\n",
        "def lower_az(s: str) -> str:\n",
        "    # Check if input is a string\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    # Normalize unicode characters\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    # Convert Turkish 'I' to 'ı' and 'İ' to 'i'\n",
        "    s = s.replace(\"I\", \"ı\").replace(\"İ\", \"i\")\n",
        "    # Standard lowercase and fix a common issue\n",
        "    s = s.lower().replace(\"i̇\", \"i\")\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex Definitions and Mappings\n",
        "\n",
        "Here we define all the regular expressions (regex) and data maps we will use to find and replace patterns in the text."
      ],
      "metadata": {
        "id": "d8JNk8DJVXaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Regex Definitions ---\n",
        "\n",
        "# Finds HTML tags (e.g., <br>, <strong>)\n",
        "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "# Finds URLs (http://... or www....)\n",
        "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
        "# Finds email addresses\n",
        "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
        "# Finds phone numbers\n",
        "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
        "# Finds user mentions (like @username)\n",
        "USER_RE = re.compile(r\"@\\w+\")\n",
        "# Finds repeated punctuation (e.g., \"!!\", \"???\")\n",
        "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
        "# Finds extra spaces\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "# Finds characters repeated 3+ times (e.g., \"sooo\")\n",
        "REPEAT_CHARS= re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE)\n",
        "\n",
        "# The main rule to find (tokenize) words, numbers, or special tags\n",
        "TOKEN_RE = re.compile(\n",
        "    r\"[A-Za-zƏəĞğIıİiÖöÜüÇçŞşXxQq]+(?:'[A-Za-zƏəĞğIıİiÖöÜüÇçŞşXxQq]+)?\"  # Words\n",
        "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO(?:POS|NEG)\"  # Tags\n",
        ")\n",
        "\n",
        "# --- Mappings ---\n",
        "\n",
        "# Map for converting emojis to tags (EMO_POS, EMO_NEG)\n",
        "EMO_MAP = {\"🙂\":\"EMO_POS\", \"😀\":\"EMO_POS\", \"😍\":\"EMO_POS\", \"😊\":\"EMO_POS\", \"👍\":\"EMO_POS\",\n",
        "           \"☹\":\"EMO_NEG\", \"🙁\":\"EMO_NEG\", \"😠\":\"EMO_NEG\", \"😡\":\"EMO_NEG\", \"👎\":\"EMO_NEG\"}\n",
        "\n",
        "# Map for correcting common slangs\n",
        "SLANG_MAP = {\"slm\":\"salam\", \"tmm\":\"tamam\", \"sagol\":\"sağol\", \"cox\":\"çox\", \"yaxsi\":\"yaxşı\"}\n",
        "\n",
        "# Words that indicate negation\n",
        "NEGATORS = {\"yox\", \"deyil\", \"heç\", \"qətiyyən\", \"yoxdur\"}"
      ],
      "metadata": {
        "id": "04UQpyDvVUp0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain-Specific Processing\n",
        "\n",
        "These functions detect the \"domain\" (topic) of the text (e.g., News, Reviews) and apply special cleaning rules only for that domain."
      ],
      "metadata": {
        "id": "aH6OXz_7WYjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Domain detection ---\n",
        "# Keywords to detect 'news' domain\n",
        "NEWS_HINTS = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
        "# Keywords/symbols for 'social' media domain\n",
        "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:😂|😍|😊|👍|👎|😡|🙂)\")\n",
        "# Keywords for 'reviews' domain\n",
        "REV_HINTS = re.compile(r\"\\b(azn|manat|qiymət|aldım|ulduz|çox yaxşı|çox pis)\\b\", re.I)\n",
        "\n",
        "# Function to check text and assign a domain\n",
        "def detect_domain(text: str) -> str:\n",
        "    s = text.lower()\n",
        "    if NEWS_HINTS.search(s): return \"news\"\n",
        "    if SOCIAL_HINTS.search(s): return \"social\"\n",
        "    if REV_HINTS.search(s): return \"reviews\"\n",
        "    return \"general\"\n",
        "\n",
        "# --- Domain-specific normalization (reviews) ---\n",
        "# Finds prices (e.g., \"10 azn\") for reviews\n",
        "PRICE_RE = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
        "# Finds star ratings (e.g., \"5 ulduz\") for reviews\n",
        "STARS_RE = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
        "# Finds positive phrases for reviews\n",
        "POS_RATE = re.compile(r\"\\bçox yaxşı\\b\")\n",
        "# Finds negative phrases for reviews\n",
        "NEG_RATE = re.compile(r\"\\bçox pis\\b\")\n",
        "\n",
        "# Function to apply special rules based on domain\n",
        "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
        "    # Only apply these rules for 'reviews'\n",
        "    if domain == \"reviews\":\n",
        "        # Replace price with <PRICE> tag\n",
        "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
        "        # Replace stars with <STARS_n> tag\n",
        "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", s)\n",
        "        # Replace positive phrase\n",
        "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
        "        # Replace negative phrase\n",
        "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
        "        # Clean up extra spaces\n",
        "        return \" \".join(s.split())\n",
        "    # Return original text if not 'reviews'\n",
        "    return cleaned\n",
        "\n",
        "# --- Domain tag token for corpus (no punctuation) ---\n",
        "# Adds a domain tag (e.g., 'domnews') to the start of a line\n",
        "def add_domain_tag(line: str, domain: str) -> str:\n",
        "    return f\"dom{domain} \" + line # e.g., 'domnews', 'domreviews'"
      ],
      "metadata": {
        "id": "d9rVkYtoWedO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Text Normalization Function\n",
        "\n",
        "This is the main function that combines all the previous helpers to perform a full text-cleaning pipeline."
      ],
      "metadata": {
        "id": "qdBudNS4Wioq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main function to clean one string of text\n",
        "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
        "\n",
        "    if not isinstance(s, str): return \"\"\n",
        "\n",
        "    # First, replace all emojis using the map\n",
        "    for emo, tag in EMO_MAP.items():\n",
        "        s = s.replace(emo, f\" {tag} \")\n",
        "\n",
        "    # Fix potential text encoding issues\n",
        "    s = fix_text(s)\n",
        "    # Convert HTML entities (like &amp;)\n",
        "    s = html.unescape(s)\n",
        "    # Remove HTML tags\n",
        "    s = HTML_TAG_RE.sub(\" \", s)\n",
        "    # Replace URLs with 'URL' token\n",
        "    s = URL_RE.sub(\" URL \", s)\n",
        "    # Replace emails with 'EMAIL' token\n",
        "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
        "    # Replace phones with 'PHONE' token\n",
        "    s = PHONE_RE.sub(\" PHONE \", s)\n",
        "    # Handle hashtags, try to split camelCase\n",
        "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" + re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \", s)\n",
        "    # Replace user mentions with 'USER' token\n",
        "    s = USER_RE.sub(\" USER \", s)\n",
        "    # Apply the Azeri-specific lowercase\n",
        "    s = lower_az(s)\n",
        "    # Reduce repeated punctuation (e.g., \"!!\" -> \"!\")\n",
        "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
        "\n",
        "    # If option is True\n",
        "    if numbers_to_token:\n",
        "        # Replace numbers with '<NUM>' token\n",
        "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
        "\n",
        "    # If keeping sentence endings\n",
        "    if keep_sentence_punct:\n",
        "        # Remove all symbols *except* sentence punctuation\n",
        "        s = re.sub(r\"[^\\w\\s<>'əğıöşüçƏĞIİÖŞÜÇxqXQ.!?]\", \" \", s)\n",
        "    else:\n",
        "        # Remove all symbols\n",
        "        s = re.sub(r\"[^\\w\\s<>'əğıöşüçƏĞIİÖŞÜÇxqXQ]\", \" \", s)\n",
        "\n",
        "    # Fix extra spaces\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "\n",
        "    # Split the clean string into a list of tokens\n",
        "    toks = TOKEN_RE.findall(s)\n",
        "\n",
        "    norm = []\n",
        "    mark_neg = 0 # Counter for negation\n",
        "\n",
        "    # Loop through each token\n",
        "    for t in toks:\n",
        "        # Reduce repeated chars (e.g., \"goood\" -> \"good\")\n",
        "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
        "        # Correct slang if found in the map\n",
        "        t = SLANG_MAP.get(t, t)\n",
        "\n",
        "        # If this token is a negation word\n",
        "        if t in NEGATORS:\n",
        "            norm.append(t);\n",
        "            # Mark the next 3 words as negative\n",
        "            mark_neg = 3;\n",
        "            continue\n",
        "\n",
        "        # If a word is marked\n",
        "        if mark_neg > 0 and t not in {\"URL\", \"EMAIL\", \"PHONE\", \"USER\"}:\n",
        "            # Add a '_NEG' suffix\n",
        "            norm.append(t + \"_NEG\");\n",
        "            mark_neg -= 1\n",
        "        else:\n",
        "            norm.append(t)\n",
        "\n",
        "    # Final small clean-up (remove single letters)\n",
        "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\", \"e\", \"ə\"})]\n",
        "    # Join tokens back into a string\n",
        "    return \" \".join(norm).strip()"
      ],
      "metadata": {
        "id": "un7J4CUCWnZ9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Value Mapping\n",
        "\n",
        "This function converts different types of labels (e.g., \"positive\", \"1\", \"mənfi\") into a single, standard numeric format (0.0, 0.5, 1.0)."
      ],
      "metadata": {
        "id": "zptR9EQLWqDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert different labels to a standard number\n",
        "def map_sentiment_value(v, scheme: str):\n",
        "\n",
        "    # For binary (0/1) classification\n",
        "    if scheme == \"binary\":\n",
        "        try: return 1.0 if int(v) == 1 else 0.0\n",
        "        except Exception: return None\n",
        "\n",
        "    s = str(v).strip().lower()\n",
        "\n",
        "    # Check for positive labels\n",
        "    if s in {\"pos\", \"positive\", \"1\", \"müsbət\", \"pozitiv\", \"good\"}: return 1.0\n",
        "    # Check for neutral labels\n",
        "    if s in {\"neu\", \"neutral\", \"2\", \"neytral\"}: return 0.5\n",
        "    # Check for negative labels\n",
        "    if s in {\"neg\", \"negative\", \"0\", \"mənfi\", \"neqativ\", \"bad\"}: return 0.0\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "gTEuk0bPWwG5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main File Processing Function\n",
        "\n",
        "This final function reads an Excel file, applies all the cleaning and mapping functions to the correct columns, and saves the result as a new two-column (text, sentiment) Excel file."
      ],
      "metadata": {
        "id": "sv4xhJjhW5N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main function to process an entire Excel file\n",
        "def process_file(in_path, text_col, label_col, scheme, out_two_col_path,\n",
        "                 remove_stopwords=False):\n",
        "\n",
        "    # Read the input Excel file\n",
        "    df = pd.read_excel(in_path)\n",
        "\n",
        "    # Remove extra columns if they exist\n",
        "    for c in [\"Unnamed: 0\", \"index\"]:\n",
        "        if c in df.columns: df = df.drop(columns=[c])\n",
        "\n",
        "    # Check that required columns exist\n",
        "    assert text_col in df.columns and label_col in df.columns, f\"Missing columns in {in_path}\"\n",
        "\n",
        "    # --- Data Cleaning Steps ---\n",
        "    # Remove rows with no text\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    # Remove rows with empty text\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
        "    # Remove duplicate rows\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "\n",
        "    # Create 'cleaned_text' column by applying main normalizer\n",
        "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s))\n",
        "\n",
        "    # Detect the domain for each text\n",
        "    df[\"__domain__\"] = df[text_col].astype(str).apply(detect_domain)\n",
        "    # Apply domain-specific normalization *after* general cleaning\n",
        "    df[\"cleaned_text\"] = df.apply(lambda r:\n",
        "                                domain_specific_normalize(r[\"cleaned_text\"], r[\"__domain__\"]), axis=1)\n",
        "\n",
        "    # If stopwatch removal is enabled\n",
        "    if remove_stopwords:\n",
        "        # Define the set of stopwords\n",
        "        sw = set([\"və\", \"ilə\", \"amma\", \"ancaq\", \"lakin\", \"ya\", \"həm\", \"artıq\", \"çox\", \"heç\",\n",
        "                  \"qətiyyən\", \"ki\", \"bu\", \"bir\", \"o\", \"biz\", \"siz\", \"sən\", \"mən\", \"az\", \"ən\",\n",
        "                  \"orada\", \"burada\", \"bütün\", \"hər\", \"də\", \"da\", \"üçün\"])\n",
        "        # Make sure *not* to remove sentiment-related words\n",
        "        for keep in [\"deyil\", \"yox\", \"yoxdur\"]:\n",
        "            sw.discard(keep)\n",
        "        # Apply the stopword removal\n",
        "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: \" \".join([t for t in\n",
        "                                                               s.split() if t not in sw]))\n",
        "\n",
        "    # Create 'sentiment_value' column by mapping labels\n",
        "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
        "    # Remove rows where sentiment could not be mapped\n",
        "    df = df.dropna(subset=[\"sentiment_value\"])\n",
        "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
        "\n",
        "    # --- Save Output ---\n",
        "    # Select only the final two columns\n",
        "    out_df = df[[\"cleaned_text\", \"sentiment_value\"]].reset_index(drop=True)\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Save the final data to a new Excel file\n",
        "    out_df.to_excel(out_two_col_path, index=False)\n",
        "\n",
        "    # Print a confirmation message\n",
        "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")"
      ],
      "metadata": {
        "id": "BsvNYw-5W8EV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus Building Function\n",
        "\n",
        "This function reads all input files, normalizes text into sentences, adds domain tags (e.g., domnews), and saves them to a single corpus_all.txt file for model training."
      ],
      "metadata": {
        "id": "dSukvPXsopsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
        "    \"\"\"Create domain-tagged, lowercase, punctuation-free corpus (one sentence per\n",
        "    line).\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for (f, text_col) in zip(input_files, text_cols):\n",
        "        df = pd.read_excel(f)\n",
        "        for raw in df[text_col].dropna().astype(str):\n",
        "            dom = detect_domain(raw)\n",
        "            # Normalize text, but keep sentence punctuation for splitting\n",
        "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
        "\n",
        "            # Split text into sentences\n",
        "            parts = re.split(r\"[.!?]+\", s)\n",
        "            for p in parts:\n",
        "                p = p.strip()\n",
        "                if not p: continue\n",
        "                # Remove remaining punctuation\n",
        "                p = re.sub(r\"[^\\w\\səğıöşüçƏĞIİÖŞÜÇxqXQ]\", \" \", p)\n",
        "                p = \" \".join(p.split()).lower()\n",
        "                if p:\n",
        "                    lines.append(f\"dom{dom} \" + p)\n",
        "\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
        "        for ln in lines:\n",
        "            w.write(ln + \"\\n\")\n",
        "    print(f\"Wrote {out_txt} with {len(lines)} lines\")"
      ],
      "metadata": {
        "id": "2gZSEnmsfBgW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution Block\n",
        "\n",
        "This block defines the input file configuration (CFG) and runs the main pipeline: processing each file into a _2col.xlsx output, and then building the combined corpus_all.txt."
      ],
      "metadata": {
        "id": "ArAN6wAWp0mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    CFG = [\n",
        "        # Using the exact filenames you provided:\n",
        "        (\"data/labeled-sentiment.xlsx\", \"text\", \"sentiment\", \"tri\"),\n",
        "        (\"data/merged_dataset_CSV__1_.xlsx\", \"text\", \"labels\", \"binary\"),\n",
        "        (\"data/test__1_.xlsx\", \"text\", \"label\", \"binary\"),\n",
        "        (\"data/train__3_.xlsx\", \"text\", \"label\", \"binary\"),\n",
        "        (\"data/train-00000-of-00001.xlsx\", \"text\", \"labels\", \"tri\"),\n",
        "    ]\n",
        "\n",
        "    # 1. Process all files into two-column (text, sentiment) outputs\n",
        "    print(\"--- Processing files for 2-column output ---\")\n",
        "    for fname, tcol, lcol, scheme in CFG:\n",
        "        # Output path now points to the 'clean_data/' folder\n",
        "        out = f\"clean_data/{Path(fname).stem}_2col.xlsx\"\n",
        "        process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)\n",
        "\n",
        "    # 2. Build the combined domain-tagged, punctuation-free corpus\n",
        "    print(\"\\n--- Building combined corpus text file ---\")\n",
        "    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG],\n",
        "                     out_txt=\"corpus_all.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xWXEq66qByq",
        "outputId": "a77a4679-3482-419d-a023-551e6e6d62eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing files for 2-column output ---\n",
            "Saved: clean_data/labeled-sentiment_2col.xlsx (rows=2955)\n",
            "Saved: clean_data/merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
            "Saved: clean_data/test__1__2col.xlsx (rows=4198)\n",
            "Saved: clean_data/train__3__2col.xlsx (rows=19557)\n",
            "Saved: clean_data/train-00000-of-00001_2col.xlsx (rows=41756)\n",
            "\n",
            "--- Building combined corpus text file ---\n",
            "Wrote corpus_all.txt with 124353 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec and FastText embedding Models\n",
        "\n",
        "Following the preprocessing steps, we now have five cleaned Excel files. The next task is to train the **Word2Vec** and **FastText** models as specified in the assignment.\n",
        "\n",
        "The code below performs the following actions:\n",
        "1.  Initializes an empty list called `sentences`.\n",
        "2.  Loops through each of the five `_2col.xlsx` files and reads the `cleaned_text` column using `pandas`.\n",
        "3.  Converts each row of cleaned text into a list of tokens (by splitting on spaces) and adds these lists to the main `sentences` collection.\n",
        "4.  Creates the `embeddings/` directory if it doesn't already exist.\n",
        "5.  Trains a `Word2Vec` model using the `sentences` corpus. Key parameters include `vector_size=300`, `window=5`, `min_count=3`, and `sg=1` (Skip-gram).\n",
        "6.  Trains a `FastText` model using the same corpus and similar parameters, but also includes subword information (`min_n=3`, `max_n=6`).\n",
        "7.  Saves both trained models to the `embeddings/` folder as `word2vec.model` and `fasttext.model`."
      ],
      "metadata": {
        "id": "raDxvkCP87Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "files = [\n",
        "    f\"./clean_data/labeled-sentiment_2col.xlsx\",\n",
        "    f\"./clean_data/test_1_2col.xlsx\",\n",
        "    f\"./clean_data/train_3_2col.xlsx\",\n",
        "    f\"./clean_data/train-00000-of-00001_2col.xlsx\",\n",
        "    f\"./clean_data/merged_dataset_CSV_1_2col.xlsx\",\n",
        "]\n",
        "\n",
        "sentences = []\n",
        "for f in files:\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
        "\n",
        "Path(\"embeddings\").mkdir(exist_ok=True)\n",
        "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1,\n",
        "negative=10, epochs=10)\n",
        "w2v.save(\"embeddings/word2vec.model\")\n",
        "ft  = FastText(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1,\n",
        "min_n=3, max_n=6, epochs=10)\n",
        "ft.save(\"embeddings/fasttext.model\")\n",
        "print(\"Saved embeddings.\")"
      ],
      "metadata": {
        "id": "lklVCJJ6xolQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation: Word2Vec vs. FastText (Quantitative & Qualitative Metrics)\n",
        "\n",
        "Model Evaluation section presents a comparative evaluation of the generated Word2Vec and FastText models. To assess their respective strengths in capturing the semantics of the Azerbaijani corpus, the analysis employs three distinct evaluation metrics.\n",
        "\n",
        "### 1. Lexical Coverage (Quantitative)\n",
        "\n",
        "This metric quantifies the **vocabulary coverage** of each model, measuring the percentage of unique tokens from our cleaned datasets that are found within the model's learned vocabulary.\n",
        "\n",
        "This is a critical test for comparing the two architectures. Word2Vec, being a word-level model, is inherently limited to its training vocabulary and cannot represent **out-of-vocabulary (OOV)** words. In contrast, FastText, which learns vectors for character n-grams (subwords), can construct vectors for *any* word, including neologisms, misspellings, or rare words not encountered during training.\n",
        "\n",
        "### 2. Semantic Similarity (Quantitative)\n",
        "\n",
        "A successful embedding model should capture meaningful **semantic relationships**, placing words with similar meanings close together in the vector space and words with opposite meanings far apart.\n",
        "\n",
        "To quantify this, we measure the average **cosine similarity** for two predefined sets of word pairs:\n",
        "* **Synonym Pairs** (e.g., `yaxşı`, `əla`): We expect a high similarity score (close to 1.0), indicating semantic proximity.\n",
        "* **Antonym Pairs** (e.g., `yaxşı`, `pis`): We expect a low or negative similarity score (close to -1.0 or 0.0), indicating semantic distance.\n",
        "\n",
        "A \"Separation Score\" (calculated as `Synonym Similarity - Antonym Similarity`) is then used to provide a single, robust measure of the model's ability to discriminate between semantic similarity and opposition. A higher separation score is better.\n",
        "\n",
        "### 3. Nearest Neighbors Analysis (Qualitative)\n",
        "\n",
        "Beyond quantitative scores, a **qualitative analysis** of the embedding space is performed by inspecting the **nearest neighbors** for a set of predefined seed words.\n",
        "\n",
        "By examining the top 5 most similar words for a given seed (e.g., `bahalı` or `pis`), we can intuitively assess the quality of the learned representations. This helps us judge whether the model has learned logical contexts (e.g., are the neighbors of \"expensive\" other price-related words?) or if it has merely learned superficial co-occurrence patterns."
      ],
      "metadata": {
        "id": "pZeYLp8x8-ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import re\n",
        "\n",
        "w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
        "ft  = FastText.load(\"embeddings/fasttext.model\")\n",
        "\n",
        "seed_words = [\"yaxşı\",\"pis\",\"çox\",\"bahalı\",\"ucuz\",\"mükəmməl\",\"dəhşət\",\"<PRICE>\",\"<RATING_POS>\"]\n",
        "syn_pairs  = [(\"yaxşı\",\"əla\"), (\"bahalı\",\"qiymətli\"), (\"ucuz\",\"sərfəli\")]\n",
        "ant_pairs  = [(\"yaxşı\",\"pis\"), (\"bahalı\",\"ucuz\")]\n",
        "\n",
        "def lexical_coverage(model, tokens):\n",
        "    vocab = model.wv.key_to_index\n",
        "    return sum(1 for t in tokens if t in vocab) / max(1,len(tokens))\n",
        "\n",
        "files = [\n",
        "    f\"./clean_data/labeled-sentiment_2col.xlsx\",\n",
        "    f\"./clean_data/test_1_2col.xlsx\",\n",
        "    f\"./clean_data/train_3_2col.xlsx\",\n",
        "    f\"./clean_data/train-00000-of-00001_2col.xlsx\",\n",
        "    f\"./clean_data/merged_dataset_CSV_1_2col.xlsx\",\n",
        "]\n",
        "\n",
        "def read_tokens(f):\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
        "\n",
        "print(\"== Lexical coverage (per dataset) ==\")\n",
        "for f in files:\n",
        "    toks = read_tokens(f)\n",
        "    cov_w2v = lexical_coverage(w2v, toks)\n",
        "    cov_ftv = lexical_coverage(ft, toks)  # FT still embeds OOV via subwords\n",
        "    print(f\"{f}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos(a,b): return float(dot(a,b)/(norm(a)*norm(b)))\n",
        "\n",
        "def pair_sim(model, pairs):\n",
        "    vals = []\n",
        "    for a,b in pairs:\n",
        "        try: vals.append(model.wv.similarity(a,b))\n",
        "        except KeyError: pass\n",
        "    return sum(vals)/len(vals) if vals else float('nan')\n",
        "\n",
        "syn_w2v = pair_sim(w2v, syn_pairs)\n",
        "syn_ft  = pair_sim(ft,  syn_pairs)\n",
        "ant_w2v = pair_sim(w2v, ant_pairs)\n",
        "ant_ft  = pair_sim(ft,  ant_pairs)\n",
        "\n",
        "print(\"\\n== Similarity (higher better for synonyms; lower better for antonyms) ==\")\n",
        "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
        "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
        "print(f\"Separation (Syn - Ant): W2V={(syn_w2v - ant_w2v):.3f}, FT={(syn_ft - ant_ft):.3f}\")\n",
        "\n",
        "def neighbors(model, word, k=5):\n",
        "  try: return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
        "  except KeyError: return []\n",
        "\n",
        "print(\"\\n== Nearest neighbors (qualitative) ==\")\n",
        "for w in seed_words:\n",
        "  print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
        "  print(f\"  FT  NN for '{w}':\", neighbors(ft,  w))\n",
        "\n",
        "# (Optional) domain drift if you train domain-specific models separately:\n",
        "# drift(word, model_a, model_b) = 1 - cos(vec_a, vec_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKK5bBK080FU",
        "outputId": "1075c73b-aa3b-43a5-cffb-77c00d695539"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Lexical coverage (per dataset) ==\n",
            "clean_data/labeled-sentiment_2col.xlsx: W2V=0.920, FT(vocab)=0.920\n",
            "clean_data/test_1_2col.xlsx: W2V=0.973, FT(vocab)=0.973\n",
            "clean_data/train_3_2col.xlsx: W2V=0.976, FT(vocab)=0.976\n",
            "clean_data/train-00000-of-00001_2col.xlsx: W2V=0.914, FT(vocab)=0.914\n",
            "clean_data/merged_dataset_CSV_1_2col.xlsx: W2V=0.929, FT(vocab)=0.929\n",
            "\n",
            "== Similarity (higher better for synonyms; lower better for antonyms) ==\n",
            "Synonyms: W2V=0.329, FT=0.452\n",
            "Antonyms: W2V=0.308, FT=0.411\n",
            "Separation (Syn - Ant): W2V=0.021, FT=0.041\n",
            "\n",
            "== Nearest neighbors (qualitative) ==\n",
            "  W2V NN for 'yaxşı': ['olardı', 'yaxshi', 'iyi', 'zor', 'olar.']\n",
            "  FT  NN for 'yaxşı': ['yaxşı-yaxşı', 'yaxşı!', 'yaxşıı', 'yaxşı)', 'yaxşıkı']\n",
            "  W2V NN for 'pis': ['<STARS_LOW>', 'pisdir', 'vərdişlərə', 'pisdi', 'bərbad']\n",
            "  FT  NN for 'pis': ['pis!', '(pis', 'pis,', 'pis.', 'piis']\n",
            "  W2V NN for 'çox': ['tətbiqidir', 'çoox', 'gözəldir', 'temu', 'çöx']\n",
            "  FT  NN for 'çox': ['çoxçox', 'çox.çox', '(çox', 'çoxx', '\"çox']\n",
            "  W2V NN for 'bahalı': [',balıq', 'portretlerinə', 'metallarla', 'radiusda', 'yaxtaları']\n",
            "  FT  NN for 'bahalı': ['bahalıı', 'bahalısı', 'bahalı,', 'bahalıq', 'baharlı']\n",
            "  W2V NN for 'ucuz': ['qiymete', 'şeytanbazardan', 'sududu', 'qiymətə', 'yeməkləri,']\n",
            "  FT  NN for 'ucuz': ['ucuza', 'ucuz.', 'ucuz,', 'ucuzu', '\"ucuz\"']\n",
            "  W2V NN for 'mükəmməl': ['süjetli', 'carrey', 'uygulama', 'keyifli', 'bayıldım']\n",
            "  FT  NN for 'mükəmməl': ['mükəmməl!', 'mükəmməl.', 'mükəmməl,', 'mükəmməll', 'mükəmməldi']\n",
            "  W2V NN for 'dəhşət': ['xalçalardan', 'kanaldır.', 'təsirlidi.', 'ayranları', 'deməyib,']\n",
            "  FT  NN for 'dəhşət': ['\"dəhşət', 'dəhşət!', 'dəhşət.', 'dəhşətdü', 'dəhşətə']\n",
            "  W2V NN for '<PRICE>': ['internetmiz', 'dolamayın', 'ödəyirik', 'siz?', 'pis!']\n",
            "  FT  NN for '<PRICE>': ['<PRICE>a', 'qr=<PRICE>.', 'vps', 'mg', '36mb']\n",
            "  W2V NN for '<RATING_POS>': []\n",
            "  FT  NN for '<RATING_POS>': ['ehali', 'nuh', 'zaryatkali', '10sm', 'agsaqqali']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}