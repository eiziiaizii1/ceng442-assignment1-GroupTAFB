{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENG442 Assignment 1 - Azerbaijani Text Preprocessing & Word Embeddings\n",
    "\n",
    "**Group Members:**\n",
    "* Talha Ubeydullah Gamga | 20050111078\n",
    "* Aziz Önder | 22050141021\n",
    "* Muhammed Fatih Asan | 23050151026\n",
    "* Buğra Bildiren | 20050111022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "In this step, we import all necessary libraries for data processing and text cleaning, including standard libraries like `pandas`, `re` (regex), and `ftfy` (for text normalization).\n",
    "\n",
    "We also import the custom utility functions (e.g., domain detection, emoji/negation handling) from the `ozel_temizlik.py` script.\n",
    "\n",
    "Finally, we define and create the `OUTPUT_DIR` (`clean_data/`) where our processed Excel files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Utility functions from 'ozel_temizlik.py' imported.\n",
      "Output directory 'clean_data' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import ftfy\n",
    "\n",
    "# --- Import Custom Utility Script ---\n",
    "# This script contains helper functions for domain detection,\n",
    "# negation, emoji mapping, and other specific cleaning tasks.\n",
    "import ozel_temizlik\n",
    "\n",
    "# --- Setup Output Directory ---\n",
    "OUTPUT_DIR = \"clean_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"Utility functions from 'ozel_temizlik.py' imported.\")\n",
    "print(f\"Output directory '{OUTPUT_DIR}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Core Helper Functions\n",
    "\n",
    "In this step, we define the core helper functions required by the main processing pipeline. These functions are responsible for:\n",
    "\n",
    "1.  **`map_sentiment_value`**: Standardizing the various sentiment labels (e.g., \"positive\", 1, 0.0) from the 5 datasets into a single numeric float format (0.0, 0.5, 1.0).\n",
    "2.  **`lower_az`**: Handling the specific lowercase conversion for Azerbaijani characters (e.g., 'İ' -> 'i', 'I' -> 'ı').\n",
    "3.  **`basic_regex_clean`**: Applying the fundamental, non-domain-specific cleaning rules (like removing HTML, normalizing URLs, Emails, Numbers) based on the code snippets provided in the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core helper functions (map_sentiment_value, lower_az, basic_regex_clean) defined.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2.1: Sentiment Label Standardization\n",
    "# (Maps all labels to 0.0, 0.5, 1.0 as float)\n",
    "# ----------------------------------------------------------------\n",
    "def map_sentiment_value(label):\n",
    "    \"\"\"\n",
    "    Converts various sentiment labels (str, int) from different\n",
    "    datasets into a standard float value (0.0, 0.5, or 1.0).\n",
    "    Returns None if the label is unmappable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle string labels\n",
    "    if isinstance(label, str):\n",
    "        label_low = label.lower().strip()\n",
    "        if label_low in ['positive', 'pos', '1']:\n",
    "            return 1.0\n",
    "        elif label_low in ['negative', 'neg', '0']:\n",
    "            return 0.0\n",
    "        elif label_low in ['neutral', 'neu', '0.5']:\n",
    "            return 0.5\n",
    "    \n",
    "    # Handle integer labels\n",
    "    if isinstance(label, int):\n",
    "        if label == 1:\n",
    "            return 1.0\n",
    "        elif label == 0:\n",
    "            return 0.0\n",
    "            \n",
    "    # Handle float labels\n",
    "    try:\n",
    "        f_label = float(label)\n",
    "        if f_label == 1.0: return 1.0\n",
    "        if f_label == 0.0: return 0.0\n",
    "        if f_label == 0.5: return 0.5\n",
    "    except (ValueError, TypeError):\n",
    "        pass \n",
    "    \n",
    "    # If no match is found\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2.2: Azerbaijani-Specific Lowercasing\n",
    "# (PDF Section 5.1.4: 'İ' -> 'i', 'I' -> 'ı')\n",
    "# ----------------------------------------------------------------\n",
    "def lower_az(text):\n",
    "    \"\"\"Applies Azerbaijani-specific lowercase conversion.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) # Ensure input is string\n",
    "    text = text.replace('İ', 'i').replace('I', 'ı')\n",
    "    return text.lower() # Apply standard lowercasing\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2.3: Basic Text Normalization (Regex)\n",
    "# (Based on PDF Section 5.1 code snippets)\n",
    "# ----------------------------------------------------------------\n",
    "def basic_regex_clean(text):\n",
    "    \"\"\"\n",
    "    Applies fundamental regex cleaning rules as specified \n",
    "    in the assignment PDF (e.g., HTML, URL, EMAIL, NUM).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fix broken Unicode (e.g., â€™ -> ’) - Recommended by PDF\n",
    "    text = ftfy.fix_text(text)\n",
    "    \n",
    "    # Remove HTML tags (PDF Section 5.1.1)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Normalize URLs (PDF Section 5.1.2)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '<URL>', text)\n",
    "    \n",
    "    # Normalize Emails (PDF Section 5.1.2)\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    \n",
    "    # Normalize @mentions (PDF Section 5.1.2)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    \n",
    "    # Normalize Phone (simple rule) (PDF Section 5.1.2)\n",
    "    # (Note: PDF has a typo r'(\\+?d... , corrected to \\d)\n",
    "    text = re.sub(r'(\\+?\\d[\\d\\s-]{7,}\\d)', '<PHONE>', text)\n",
    "    \n",
    "    # Normalize Numbers (as per PDF Section 5.1.6)\n",
    "    text = re.sub(r'\\b\\d+[\\.,\\d]*\\b', '<NUM>', text)\n",
    "    \n",
    "    # Normalize repeating characters (e.g., çooox -> çoxx) (PDF Section 5.1.6)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Core helper functions (map_sentiment_value, lower_az, basic_regex_clean) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Main Normalization Pipeline (normalize_text_az)\n",
    "\n",
    "This is the main \"glue\" function for our pipeline. It's responsible for executing all cleaning steps in the correct logical order.\n",
    "\n",
    "It combines the **basic** cleaning functions (defined in Step 2, e.g., `basic_regex_clean`, `lower_az`) with the **advanced/specialized** functions imported from `ozel_temizlik.py` (e.g., `split_hashtags`, `handle_negation`).\n",
    "\n",
    "The main `process_file` function (which we will use in the next step) will call this single function to perform the complete text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main pipeline function 'normalize_text_az' defined.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 3.1: Main Normalization Pipeline Function\n",
    "# (This function is called by the PDF's process_file skeleton)\n",
    "# ----------------------------------------------------------------\n",
    "def normalize_text_az(raw_text, domain):\n",
    "    \"\"\"\n",
    "    Applies the full sequence of cleaning and normalization steps.\n",
    "    \n",
    "    This function combines the basic regex cleaning with the \n",
    "    domain-specific and special challenge functions in a logical order.\n",
    "    \n",
    "    Args:\n",
    "        raw_text (str): The original, unprocessed text.\n",
    "        domain (str): The detected domain ('news', 'social', etc.).\n",
    "    \n",
    "    Returns:\n",
    "        str: The fully cleaned and normalized text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Basic Azeri Lowercasing (from Step 2)\n",
    "    text = lower_az(raw_text)\n",
    "    \n",
    "    # Step 2: Basic Regex Cleaning (HTML, URL, NUM, etc.) (from Step 2)\n",
    "    text = basic_regex_clean(text)\n",
    "    \n",
    "    # --- Apply special functions from ozel_temizlik.py ---\n",
    "    \n",
    "    # Step 3: Split CamelCase hashtags (e.g., #QarabagIsBack -> qarabag is back)\n",
    "    text = ozel_temizlik.split_hashtags(text)\n",
    "    \n",
    "    # Step 4: Map Emojis (e.g., :) -> EMO_POS)\n",
    "    text = ozel_temizlik.map_emojis_and_normalize(text)\n",
    "    \n",
    "    # Step 5: Deasciify/Slang (e.g., cox -> çox)\n",
    "    text = ozel_temizlik.deasciify_slang(text)\n",
    "    \n",
    "    # Step 6: Domain-Specific Normalization (e.g., 50 azn -> <PRICE>)\n",
    "    # (Must run BEFORE negation to avoid tagging <PRICE>_NEG)\n",
    "    text = ozel_temizlik.domain_specific_normalize(text, domain)\n",
    "    \n",
    "    # Step 7: Handle Negation (e.g., \"yoxdur\" -> \"yoxdur\") \n",
    "    # (Note: Negation function in ozel_temizlik.py handles _NEG tagging)\n",
    "    text = ozel_temizlik.handle_negation(text)\n",
    "    \n",
    "    # Step 8: Final cleanup (remove extra whitespace created during cleaning)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Main pipeline function 'normalize_text_az' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Main Processing Function and Dataset List\n",
    "\n",
    "Now we define the final pieces needed to run the entire pipeline:\n",
    "\n",
    "1.  **`datasets_to_process`**: A list of dictionaries defining the 5 raw data files to process. This list, based on Section 2 of the PDF, specifies the input file path, the column names for text and label (which differ between files), and the final output path.\n",
    "2.  **`process_file`**: The main function skeleton provided in the assignment PDF (Section 7.1). This function reads a file, applies all our helper functions (`map_sentiment_value`, `normalize_text_az`, etc.) in the correct order, removes duplicates/empties, and saves the final two-column Excel file to our `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset list and main 'process_file' function defined.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 4.1: Define the list of datasets to process\n",
    "# (Based on PDF Section 2)\n",
    "# ----------------------------------------------------------------\n",
    "datasets_to_process = [\n",
    "    {\n",
    "        \"in_file\": \"data/labeled-sentiment.xlsx\",\n",
    "        \"text_col\": \"text\",\n",
    "        \"label_col\": \"sentiment\",\n",
    "        \"out_file\": os.path.join(OUTPUT_DIR, \"labeled-sentiment_2col.xlsx\")\n",
    "    },\n",
    "    {\n",
    "        \"in_file\": \"data/test__1_.xlsx\",\n",
    "        \"text_col\": \"text\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"out_file\": os.path.join(OUTPUT_DIR, \"test_1_2col.xlsx\") # Using PDF canonical name\n",
    "    },\n",
    "    {\n",
    "        \"in_file\": \"data/train__3_.xlsx\",\n",
    "        \"text_col\": \"text\",\n",
    "        \"label_col\": \"label\",\n",
    "        \"out_file\": os.path.join(OUTPUT_DIR, \"train_3_2col.xlsx\") # Using PDF canonical name\n",
    "    },\n",
    "    {\n",
    "        \"in_file\": \"data/train-00000-of-00001.xlsx\",\n",
    "        \"text_col\": \"text\",\n",
    "        \"label_col\": \"labels\",\n",
    "        \"out_file\": os.path.join(OUTPUT_DIR, \"train-00000-of-00001_2col.xlsx\")\n",
    "    },\n",
    "    {\n",
    "        \"in_file\": \"data/merged_dataset_CSV__1_.xlsx\",\n",
    "        \"text_col\": \"text\",\n",
    "        \"label_col\": \"labels\",\n",
    "        \"out_file\": os.path.join(OUTPUT_DIR, \"merged_dataset_CSV_1_2col.xlsx\") # Using PDF canonical name\n",
    "    }\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4.2: Define the main processing function\n",
    "# (Based on the skeleton from PDF Section 7.1)\n",
    "# ----------------------------------------------------------------\n",
    "def process_file(in_file, text_col, label_col, out_file):\n",
    "    \"\"\"\n",
    "    Reads a raw dataset, applies the full cleaning pipeline,\n",
    "    and saves the required two-column (cleaned_text, sentiment_value)\n",
    "    Excel file.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing: {in_file}...\")\n",
    "    try:\n",
    "        # 1. Read Data\n",
    "        df = pd.read_excel(in_file)\n",
    "        \n",
    "        # 2. Drop rows with missing text or labels (PDF Section 5.1.7)\n",
    "        df.dropna(subset=[text_col, label_col], inplace=True)\n",
    "        \n",
    "        # 3. Ensure text column is string\n",
    "        df[text_col] = df[text_col].astype(str)\n",
    "        \n",
    "        # 4. Drop duplicate texts (PDF Section 5.1.7)\n",
    "        df.drop_duplicates(subset=[text_col], inplace=True)\n",
    "        \n",
    "        # 5. Map sentiment labels (Using our function from Step 2)\n",
    "        df['sentiment_value'] = df[label_col].apply(map_sentiment_value)\n",
    "        \n",
    "        # 6. Drop rows where mapping failed (e.g., unmappable labels)\n",
    "        df.dropna(subset=['sentiment_value'], inplace=True)\n",
    "        \n",
    "        # 7. Detect domain (Using function from ozel_temizlik.py)\n",
    "        # (This must be done on the *raw text* to catch hashtags, URLs, etc.)\n",
    "        df['domain'] = df[text_col].apply(ozel_temizlik.detect_domain)\n",
    "        \n",
    "        # 8. Apply the main normalization pipeline (Using our function from Step 3)\n",
    "        print(\"Applying normalization pipeline...\")\n",
    "        df['cleaned_text'] = df.apply(\n",
    "            lambda row: normalize_text_az(row[text_col], row['domain']), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # 9. Select only the required two columns\n",
    "        final_df = df[['cleaned_text', 'sentiment_value']]\n",
    "        \n",
    "        # 10. Save to Excel\n",
    "        final_df.to_excel(out_file, index=False, engine='openpyxl')\n",
    "        \n",
    "        print(f\"SUCCESS: Saved {len(final_df)} rows to {out_file}\")\n",
    "        return len(final_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERROR processing {in_file}: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(\"Dataset list and main 'process_file' function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Execute the Processing Pipeline\n",
    "\n",
    "Now that all helper functions, the main normalization pipeline (`normalize_text_az`), and the main processing function (`process_file`) are defined, we can execute the process.\n",
    "\n",
    "This final step iterates through the `datasets_to_process` list (defined in Step 4) and calls the `process_file` function for each dataset.\n",
    "\n",
    "The output will be 5 separate `.xlsx` files, saved in the `clean_data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the processing of 5 datasets...\n",
      "\n",
      "Processing: data/labeled-sentiment.xlsx...\n",
      "Applying normalization pipeline...\n",
      "SUCCESS: Saved 2955 rows to clean_data/labeled-sentiment_2col.xlsx\n",
      "\n",
      "Processing: data/test__1_.xlsx...\n",
      "Applying normalization pipeline...\n",
      "SUCCESS: Saved 4198 rows to clean_data/test_1_2col.xlsx\n",
      "\n",
      "Processing: data/train__3_.xlsx...\n",
      "Applying normalization pipeline...\n",
      "SUCCESS: Saved 19557 rows to clean_data/train_3_2col.xlsx\n",
      "\n",
      "Processing: data/train-00000-of-00001.xlsx...\n",
      "Applying normalization pipeline...\n",
      "SUCCESS: Saved 41756 rows to clean_data/train-00000-of-00001_2col.xlsx\n",
      "\n",
      "Processing: data/merged_dataset_CSV__1_.xlsx...\n",
      "Applying normalization pipeline...\n",
      "SUCCESS: Saved 55662 rows to clean_data/merged_dataset_CSV_1_2col.xlsx\n",
      "\n",
      "==============================\n",
      "ALL PROCESSING COMPLETE.\n",
      "Total rows processed across all files: 124128\n",
      "Please check the 'clean_data' directory for the 5 Excel files.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 5.1: Run the main processing loop\n",
    "# ----------------------------------------------------------------\n",
    "print(f\"Starting the processing of {len(datasets_to_process)} datasets...\")\n",
    "total_rows_processed = 0\n",
    "\n",
    "for dataset in datasets_to_process:\n",
    "    # Call the main function defined in Step 4\n",
    "    rows = process_file(\n",
    "        in_file=dataset[\"in_file\"],\n",
    "        text_col=dataset[\"text_col\"],\n",
    "        label_col=dataset[\"label_col\"],\n",
    "        out_file=dataset[\"out_file\"]\n",
    "    )\n",
    "    total_rows_processed += rows\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"ALL PROCESSING COMPLETE.\")\n",
    "print(f\"Total rows processed across all files: {total_rows_processed}\")\n",
    "print(f\"Please check the '{OUTPUT_DIR}' directory for the 5 Excel files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOP5ho6E9SVvdzyJbpekIxs",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
