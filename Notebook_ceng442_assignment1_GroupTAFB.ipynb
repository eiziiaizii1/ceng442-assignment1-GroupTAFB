{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eiziiaizii1/ceng442-assignment1-GroupTAFB/blob/main/Notebook_ceng442_assignment1_GroupTAFB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYv14tsAwLov"
      },
      "source": [
        "# CENG442 Assignment 1 - Azerbaijani Text Preprocessing & Word Embeddings\n",
        "\n",
        "**Group Members:**\n",
        "* Talha Ubeydullah Gamga | 20050111078\n",
        "* Aziz √ñnder | 22050141021\n",
        "* Muhammed Fatih Asan | 23050151026\n",
        "* Buƒüra Bildiren | 20050111022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhKFLV_wLox"
      },
      "source": [
        "This notebook contains a full pipeline for cleaning, normalizing, and preparing Azerbaijani text data for machine learning, with a special focus on sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL INITIALLY, IF YOU ARE RUNNING IN COLAB\n",
        "!git clone https://github.com/eiziiaizii1/ceng442-assignment1-GroupTAFB.git\n",
        "%cd ceng442-assignment1-GroupTAFB\n",
        "!pip install pandas gensim openpyxl regex ftfy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMK10BjM0aA",
        "outputId": "ec2f0702-d772-4334-ccc2-8ff0e54813f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ceng442-assignment1-GroupTAFB' already exists and is not an empty directory.\n",
            "/content/ceng442-assignment1-GroupTAFB\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (6.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R6zkzmgwLoy"
      },
      "outputs": [],
      "source": [
        "import re, html, unicodedata\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import 'ftfy' for fixing text encoding\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "# If 'ftfy' is not installed, create a dummy function\n",
        "except Exception:\n",
        "    def fix_text(s): return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRZ8bJyCwLo0"
      },
      "source": [
        "## Core Normalization Helpers\n",
        "\n",
        "First, we define a function for language-specific lowercasing, as Azerbaijani has unique 'i' and 'I' characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4QxYQTnwLo0"
      },
      "outputs": [],
      "source": [
        "# Azerbaijani-aware lowercase\n",
        "def lower_az(s: str) -> str:\n",
        "    # Check if input is a string\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    # Normalize unicode characters\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "    # Convert Turkish 'I' to 'ƒ±' and 'ƒ∞' to 'i'\n",
        "    s = s.replace(\"I\", \"ƒ±\").replace(\"ƒ∞\", \"i\")\n",
        "    # Standard lowercase and fix a common issue\n",
        "    s = s.lower().replace(\"iÃá\", \"i\")\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex Definitions and Mappings\n",
        "\n",
        "Here we define all the regular expressions (regex) and data maps we will use to find and replace patterns in the text."
      ],
      "metadata": {
        "id": "d8JNk8DJVXaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Regex Definitions ---\n",
        "\n",
        "# Finds HTML tags (e.g., <br>, <strong>)\n",
        "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "# Finds URLs (http://... or www....)\n",
        "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", re.IGNORECASE)\n",
        "# Finds email addresses\n",
        "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", re.IGNORECASE)\n",
        "# Finds phone numbers\n",
        "PHONE_RE = re.compile(r\"\\+?\\d[\\d\\-\\s\\(\\)]{6,}\\d\")\n",
        "# Finds user mentions (like @username)\n",
        "USER_RE = re.compile(r\"@\\w+\")\n",
        "# Finds repeated punctuation (e.g., \"!!\", \"???\")\n",
        "MULTI_PUNCT = re.compile(r\"([!?.,;:])\\1{1,}\")\n",
        "# Finds extra spaces\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "# Finds characters repeated 3+ times (e.g., \"sooo\")\n",
        "REPEAT_CHARS= re.compile(r\"(.)\\1{2,}\", flags=re.UNICODE)\n",
        "\n",
        "# The main rule to find (tokenize) words, numbers, or special tags\n",
        "TOKEN_RE = re.compile(\n",
        "    r\"[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+(?:'[A-Za-z∆è…ôƒûƒüIƒ±ƒ∞i√ñ√∂√ú√º√á√ß≈û≈üXxQq]+)?\"  # Words\n",
        "    r\"|<NUM>|URL|EMAIL|PHONE|USER|EMO(?:POS|NEG)\"  # Tags\n",
        ")\n",
        "\n",
        "# --- Mappings ---\n",
        "\n",
        "# Map for converting emojis to tags (EMO_POS, EMO_NEG)\n",
        "EMO_MAP = {\"üôÇ\":\"EMO_POS\", \"üòÄ\":\"EMO_POS\", \"üòç\":\"EMO_POS\", \"üòä\":\"EMO_POS\", \"üëç\":\"EMO_POS\",\n",
        "           \"‚òπ\":\"EMO_NEG\", \"üôÅ\":\"EMO_NEG\", \"üò†\":\"EMO_NEG\", \"üò°\":\"EMO_NEG\", \"üëé\":\"EMO_NEG\"}\n",
        "\n",
        "# Map for correcting common slangs\n",
        "SLANG_MAP = {\"slm\":\"salam\", \"tmm\":\"tamam\", \"sagol\":\"saƒüol\", \"cox\":\"√ßox\", \"yaxsi\":\"yax≈üƒ±\"}\n",
        "\n",
        "# Words that indicate negation\n",
        "NEGATORS = {\"yox\", \"deyil\", \"he√ß\", \"q…ôtiyy…ôn\", \"yoxdur\"}"
      ],
      "metadata": {
        "id": "04UQpyDvVUp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain-Specific Processing\n",
        "\n",
        "These functions detect the \"domain\" (topic) of the text (e.g., News, Reviews) and apply special cleaning rules only for that domain."
      ],
      "metadata": {
        "id": "aH6OXz_7WYjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Domain detection ---\n",
        "# Keywords to detect 'news' domain\n",
        "NEWS_HINTS = re.compile(r\"\\b(apa|trend|azertac|reuters|bloomberg|dha|aa)\\b\", re.I)\n",
        "# Keywords/symbols for 'social' media domain\n",
        "SOCIAL_HINTS = re.compile(r\"\\b(rt)\\b|@|#|(?:üòÇ|üòç|üòä|üëç|üëé|üò°|üôÇ)\")\n",
        "# Keywords for 'reviews' domain\n",
        "REV_HINTS = re.compile(r\"\\b(azn|manat|qiym…ôt|aldƒ±m|ulduz|√ßox yax≈üƒ±|√ßox pis)\\b\", re.I)\n",
        "\n",
        "# Function to check text and assign a domain\n",
        "def detect_domain(text: str) -> str:\n",
        "    s = text.lower()\n",
        "    if NEWS_HINTS.search(s): return \"news\"\n",
        "    if SOCIAL_HINTS.search(s): return \"social\"\n",
        "    if REV_HINTS.search(s): return \"reviews\"\n",
        "    return \"general\"\n",
        "\n",
        "# --- Domain-specific normalization (reviews) ---\n",
        "# Finds prices (e.g., \"10 azn\") for reviews\n",
        "PRICE_RE = re.compile(r\"\\b\\d+\\s*(azn|manat)\\b\", re.I)\n",
        "# Finds star ratings (e.g., \"5 ulduz\") for reviews\n",
        "STARS_RE = re.compile(r\"\\b([1-5])\\s*ulduz\\b\", re.I)\n",
        "# Finds positive phrases for reviews\n",
        "POS_RATE = re.compile(r\"\\b√ßox yax≈üƒ±\\b\")\n",
        "# Finds negative phrases for reviews\n",
        "NEG_RATE = re.compile(r\"\\b√ßox pis\\b\")\n",
        "\n",
        "# Function to apply special rules based on domain\n",
        "def domain_specific_normalize(cleaned: str, domain: str) -> str:\n",
        "    # Only apply these rules for 'reviews'\n",
        "    if domain == \"reviews\":\n",
        "        # Replace price with <PRICE> tag\n",
        "        s = PRICE_RE.sub(\" <PRICE> \", cleaned)\n",
        "        # Replace stars with <STARS_n> tag\n",
        "        s = STARS_RE.sub(lambda m: f\" <STARS_{m.group(1)}> \", s)\n",
        "        # Replace positive phrase\n",
        "        s = POS_RATE.sub(\" <RATING_POS> \", s)\n",
        "        # Replace negative phrase\n",
        "        s = NEG_RATE.sub(\" <RATING_NEG> \", s)\n",
        "        # Clean up extra spaces\n",
        "        return \" \".join(s.split())\n",
        "    # Return original text if not 'reviews'\n",
        "    return cleaned\n",
        "\n",
        "# --- Domain tag token for corpus (no punctuation) ---\n",
        "# Adds a domain tag (e.g., 'domnews') to the start of a line\n",
        "def add_domain_tag(line: str, domain: str) -> str:\n",
        "    return f\"dom{domain} \" + line # e.g., 'domnews', 'domreviews'"
      ],
      "metadata": {
        "id": "d9rVkYtoWedO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Text Normalization Function\n",
        "\n",
        "This is the main function that combines all the previous helpers to perform a full text-cleaning pipeline."
      ],
      "metadata": {
        "id": "qdBudNS4Wioq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main function to clean one string of text\n",
        "def normalize_text_az(s: str, numbers_to_token=True, keep_sentence_punct=False) -> str:\n",
        "\n",
        "    if not isinstance(s, str): return \"\"\n",
        "\n",
        "    # First, replace all emojis using the map\n",
        "    for emo, tag in EMO_MAP.items():\n",
        "        s = s.replace(emo, f\" {tag} \")\n",
        "\n",
        "    # Fix potential text encoding issues\n",
        "    s = fix_text(s)\n",
        "    # Convert HTML entities (like &amp;)\n",
        "    s = html.unescape(s)\n",
        "    # Remove HTML tags\n",
        "    s = HTML_TAG_RE.sub(\" \", s)\n",
        "    # Replace URLs with 'URL' token\n",
        "    s = URL_RE.sub(\" URL \", s)\n",
        "    # Replace emails with 'EMAIL' token\n",
        "    s = EMAIL_RE.sub(\" EMAIL \", s)\n",
        "    # Replace phones with 'PHONE' token\n",
        "    s = PHONE_RE.sub(\" PHONE \", s)\n",
        "    # Handle hashtags, try to split camelCase\n",
        "    s = re.sub(r\"#([A-Za-z0-9_]+)\", lambda m: \" \" + re.sub('([a-z])([A-Z])', r'\\1 \\2', m.group(1)) + \" \", s)\n",
        "    # Replace user mentions with 'USER' token\n",
        "    s = USER_RE.sub(\" USER \", s)\n",
        "    # Apply the Azeri-specific lowercase\n",
        "    s = lower_az(s)\n",
        "    # Reduce repeated punctuation (e.g., \"!!\" -> \"!\")\n",
        "    s = MULTI_PUNCT.sub(r\"\\1\", s)\n",
        "\n",
        "    # If option is True\n",
        "    if numbers_to_token:\n",
        "        # Replace numbers with '<NUM>' token\n",
        "        s = re.sub(r\"\\d+\", \" <NUM> \", s)\n",
        "\n",
        "    # If keeping sentence endings\n",
        "    if keep_sentence_punct:\n",
        "        # Remove all symbols *except* sentence punctuation\n",
        "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ.!?]\", \" \", s)\n",
        "    else:\n",
        "        # Remove all symbols\n",
        "        s = re.sub(r\"[^\\w\\s<>'…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", s)\n",
        "\n",
        "    # Fix extra spaces\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "\n",
        "    # Split the clean string into a list of tokens\n",
        "    toks = TOKEN_RE.findall(s)\n",
        "\n",
        "    norm = []\n",
        "    mark_neg = 0 # Counter for negation\n",
        "\n",
        "    # Loop through each token\n",
        "    for t in toks:\n",
        "        # Reduce repeated chars (e.g., \"goood\" -> \"good\")\n",
        "        t = REPEAT_CHARS.sub(r\"\\1\\1\", t)\n",
        "        # Correct slang if found in the map\n",
        "        t = SLANG_MAP.get(t, t)\n",
        "\n",
        "        # If this token is a negation word\n",
        "        if t in NEGATORS:\n",
        "            norm.append(t);\n",
        "            # Mark the next 3 words as negative\n",
        "            mark_neg = 3;\n",
        "            continue\n",
        "\n",
        "        # If a word is marked\n",
        "        if mark_neg > 0 and t not in {\"URL\", \"EMAIL\", \"PHONE\", \"USER\"}:\n",
        "            # Add a '_NEG' suffix\n",
        "            norm.append(t + \"_NEG\");\n",
        "            mark_neg -= 1\n",
        "        else:\n",
        "            norm.append(t)\n",
        "\n",
        "    # Final small clean-up (remove single letters)\n",
        "    norm = [t for t in norm if not (len(t) == 1 and t not in {\"o\", \"e\", \"…ô\"})]\n",
        "    # Join tokens back into a string\n",
        "    return \" \".join(norm).strip()"
      ],
      "metadata": {
        "id": "un7J4CUCWnZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Value Mapping\n",
        "\n",
        "This function converts different types of labels (e.g., \"positive\", \"1\", \"m…ônfi\") into a single, standard numeric format (0.0, 0.5, 1.0)."
      ],
      "metadata": {
        "id": "zptR9EQLWqDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert different labels to a standard number\n",
        "def map_sentiment_value(v, scheme: str):\n",
        "\n",
        "    # For binary (0/1) classification\n",
        "    if scheme == \"binary\":\n",
        "        try: return 1.0 if int(v) == 1 else 0.0\n",
        "        except Exception: return None\n",
        "\n",
        "    s = str(v).strip().lower()\n",
        "\n",
        "    # Check for positive labels\n",
        "    if s in {\"pos\", \"positive\", \"1\", \"m√ºsb…ôt\", \"pozitiv\", \"good\"}: return 1.0\n",
        "    # Check for neutral labels\n",
        "    if s in {\"neu\", \"neutral\", \"2\", \"neytral\"}: return 0.5\n",
        "    # Check for negative labels\n",
        "    if s in {\"neg\", \"negative\", \"0\", \"m…ônfi\", \"neqativ\", \"bad\"}: return 0.0\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "gTEuk0bPWwG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main File Processing Function\n",
        "\n",
        "This final function reads an Excel file, applies all the cleaning and mapping functions to the correct columns, and saves the result as a new two-column (text, sentiment) Excel file."
      ],
      "metadata": {
        "id": "sv4xhJjhW5N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The main function to process an entire Excel file\n",
        "def process_file(in_path, text_col, label_col, scheme, out_two_col_path,\n",
        "                 remove_stopwords=False):\n",
        "\n",
        "    # Read the input Excel file\n",
        "    df = pd.read_excel(in_path)\n",
        "\n",
        "    # Remove extra columns if they exist\n",
        "    for c in [\"Unnamed: 0\", \"index\"]:\n",
        "        if c in df.columns: df = df.drop(columns=[c])\n",
        "\n",
        "    # Check that required columns exist\n",
        "    assert text_col in df.columns and label_col in df.columns, f\"Missing columns in {in_path}\"\n",
        "\n",
        "    # --- Data Cleaning Steps ---\n",
        "    # Remove rows with no text\n",
        "    df = df.dropna(subset=[text_col])\n",
        "    # Remove rows with empty text\n",
        "    df = df[df[text_col].astype(str).str.strip().str.len() > 0]\n",
        "    # Remove duplicate rows\n",
        "    df = df.drop_duplicates(subset=[text_col])\n",
        "\n",
        "    # Create 'cleaned_text' column by applying main normalizer\n",
        "    df[\"cleaned_text\"] = df[text_col].astype(str).apply(lambda s: normalize_text_az(s))\n",
        "\n",
        "    # Detect the domain for each text\n",
        "    df[\"__domain__\"] = df[text_col].astype(str).apply(detect_domain)\n",
        "    # Apply domain-specific normalization *after* general cleaning\n",
        "    df[\"cleaned_text\"] = df.apply(lambda r:\n",
        "                                domain_specific_normalize(r[\"cleaned_text\"], r[\"__domain__\"]), axis=1)\n",
        "\n",
        "    # If stopwatch removal is enabled\n",
        "    if remove_stopwords:\n",
        "        # Define the set of stopwords\n",
        "        sw = set([\"v…ô\", \"il…ô\", \"amma\", \"ancaq\", \"lakin\", \"ya\", \"h…ôm\", \"artƒ±q\", \"√ßox\", \"he√ß\",\n",
        "                  \"q…ôtiyy…ôn\", \"ki\", \"bu\", \"bir\", \"o\", \"biz\", \"siz\", \"s…ôn\", \"m…ôn\", \"az\", \"…ôn\",\n",
        "                  \"orada\", \"burada\", \"b√ºt√ºn\", \"h…ôr\", \"d…ô\", \"da\", \"√º√ß√ºn\"])\n",
        "        # Make sure *not* to remove sentiment-related words\n",
        "        for keep in [\"deyil\", \"yox\", \"yoxdur\"]:\n",
        "            sw.discard(keep)\n",
        "        # Apply the stopword removal\n",
        "        df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda s: \" \".join([t for t in\n",
        "                                                               s.split() if t not in sw]))\n",
        "\n",
        "    # Create 'sentiment_value' column by mapping labels\n",
        "    df[\"sentiment_value\"] = df[label_col].apply(lambda v: map_sentiment_value(v, scheme))\n",
        "    # Remove rows where sentiment could not be mapped\n",
        "    df = df.dropna(subset=[\"sentiment_value\"])\n",
        "    df[\"sentiment_value\"] = df[\"sentiment_value\"].astype(float)\n",
        "\n",
        "    # --- Save Output ---\n",
        "    # Select only the final two columns\n",
        "    out_df = df[[\"cleaned_text\", \"sentiment_value\"]].reset_index(drop=True)\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    Path(out_two_col_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    # Save the final data to a new Excel file\n",
        "    out_df.to_excel(out_two_col_path, index=False)\n",
        "\n",
        "    # Print a confirmation message\n",
        "    print(f\"Saved: {out_two_col_path} (rows={len(out_df)})\")"
      ],
      "metadata": {
        "id": "BsvNYw-5W8EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus Building Function\n",
        "\n",
        "This function reads all input files, normalizes text into sentences, adds domain tags (e.g., domnews), and saves them to a single corpus_all.txt file for model training."
      ],
      "metadata": {
        "id": "dSukvPXsopsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus_txt(input_files, text_cols, out_txt=\"corpus_all.txt\"):\n",
        "    \"\"\"Create domain-tagged, lowercase, punctuation-free corpus (one sentence per\n",
        "    line).\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for (f, text_col) in zip(input_files, text_cols):\n",
        "        df = pd.read_excel(f)\n",
        "        for raw in df[text_col].dropna().astype(str):\n",
        "            dom = detect_domain(raw)\n",
        "            # Normalize text, but keep sentence punctuation for splitting\n",
        "            s = normalize_text_az(raw, keep_sentence_punct=True)\n",
        "\n",
        "            # Split text into sentences\n",
        "            parts = re.split(r\"[.!?]+\", s)\n",
        "            for p in parts:\n",
        "                p = p.strip()\n",
        "                if not p: continue\n",
        "                # Remove remaining punctuation\n",
        "                p = re.sub(r\"[^\\w\\s…ôƒüƒ±√∂≈ü√º√ß∆èƒûIƒ∞√ñ≈û√ú√áxqXQ]\", \" \", p)\n",
        "                p = \" \".join(p.split()).lower()\n",
        "                if p:\n",
        "                    lines.append(f\"dom{dom} \" + p)\n",
        "\n",
        "    with open(out_txt, \"w\", encoding=\"utf-8\") as w:\n",
        "        for ln in lines:\n",
        "            w.write(ln + \"\\n\")\n",
        "    print(f\"Wrote {out_txt} with {len(lines)} lines\")"
      ],
      "metadata": {
        "id": "2gZSEnmsfBgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution Block\n",
        "\n",
        "This block defines the input file configuration (CFG) and runs the main pipeline: processing each file into a _2col.xlsx output, and then building the combined corpus_all.txt."
      ],
      "metadata": {
        "id": "ArAN6wAWp0mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    CFG = [\n",
        "        # Using the exact filenames you provided:\n",
        "        (\"data/labeled-sentiment.xlsx\", \"text\", \"sentiment\", \"tri\"),\n",
        "        (\"data/merged_dataset_CSV__1_.xlsx\", \"text\", \"labels\", \"binary\"),\n",
        "        (\"data/test__1_.xlsx\", \"text\", \"label\", \"binary\"),\n",
        "        (\"data/train__3_.xlsx\", \"text\", \"label\", \"binary\"),\n",
        "        (\"data/train-00000-of-00001.xlsx\", \"text\", \"labels\", \"tri\"),\n",
        "    ]\n",
        "\n",
        "    # 1. Process all files into two-column (text, sentiment) outputs\n",
        "    print(\"--- Processing files for 2-column output ---\")\n",
        "    for fname, tcol, lcol, scheme in CFG:\n",
        "        # Output path now points to the 'clean_data/' folder\n",
        "        out = f\"clean_data/{Path(fname).stem}_2col.xlsx\"\n",
        "        process_file(fname, tcol, lcol, scheme, out, remove_stopwords=False)\n",
        "\n",
        "    # 2. Build the combined domain-tagged, punctuation-free corpus\n",
        "    print(\"\\n--- Building combined corpus text file ---\")\n",
        "    build_corpus_txt([c[0] for c in CFG], [c[1] for c in CFG],\n",
        "                     out_txt=\"corpus_all.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xWXEq66qByq",
        "outputId": "eb066d9d-9021-49fb-d99a-8b142a0f7850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing files for 2-column output ---\n",
            "Saved: clean_data/labeled-sentiment_2col.xlsx (rows=2955)\n",
            "Saved: clean_data/merged_dataset_CSV__1__2col.xlsx (rows=55662)\n",
            "Saved: clean_data/test__1__2col.xlsx (rows=4198)\n",
            "Saved: clean_data/train__3__2col.xlsx (rows=19557)\n",
            "Saved: clean_data/train-00000-of-00001_2col.xlsx (rows=41756)\n",
            "\n",
            "--- Building combined corpus text file ---\n",
            "Wrote corpus_all.txt with 124353 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec and FastText embedding Models\n",
        "\n",
        "Following the preprocessing steps, we now have five cleaned Excel files. The next task is to train the **Word2Vec** and **FastText** models as specified in the assignment.\n",
        "\n",
        "The code below performs the following actions:\n",
        "1.  Initializes an empty list called `sentences`.\n",
        "2.  Loops through each of the five `_2col.xlsx` files and reads the `cleaned_text` column using `pandas`.\n",
        "3.  Converts each row of cleaned text into a list of tokens (by splitting on spaces) and adds these lists to the main `sentences` collection.\n",
        "4.  Creates the `embeddings/` directory if it doesn't already exist.\n",
        "5.  Trains a `Word2Vec` model using the `sentences` corpus. Key parameters include `vector_size=300`, `window=5`, `min_count=3`, and `sg=1` (Skip-gram).\n",
        "6.  Trains a `FastText` model using the same corpus and similar parameters, but also includes subword information (`min_n=3`, `max_n=6`).\n",
        "7.  Saves both trained models to the `embeddings/` folder as `word2vec.model` and `fasttext.model`."
      ],
      "metadata": {
        "id": "raDxvkCP87Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "files = [\n",
        "    f\"./clean_data/labeled-sentiment_2col.xlsx\",\n",
        "    f\"./clean_data/test_1_2col.xlsx\",\n",
        "    f\"./clean_data/train_3_2col.xlsx\",\n",
        "    f\"./clean_data/train-00000-of-00001_2col.xlsx\",\n",
        "    f\"./clean_data/merged_dataset_CSV_1_2col.xlsx\",\n",
        "]\n",
        "\n",
        "sentences = []\n",
        "for f in files:\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    sentences.extend(df[\"cleaned_text\"].astype(str).str.split().tolist())\n",
        "\n",
        "Path(\"embeddings\").mkdir(exist_ok=True)\n",
        "w2v = Word2Vec(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1,\n",
        "negative=10, epochs=10)\n",
        "w2v.save(\"embeddings/word2vec.model\")\n",
        "ft  = FastText(sentences=sentences, vector_size=300, window=5, min_count=3, sg=1,\n",
        "min_n=3, max_n=6, epochs=10)\n",
        "ft.save(\"embeddings/fasttext.model\")\n",
        "print(\"Saved embeddings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lklVCJJ6xolQ",
        "outputId": "f1674b31-21f3-4564-a40e-824fd5128e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation: Word2Vec vs. FastText (Quantitative & Qualitative Metrics)\n",
        "\n",
        "Model Evaluation section presents a comparative evaluation of the generated Word2Vec and FastText models. To assess their respective strengths in capturing the semantics of the Azerbaijani corpus, the analysis employs three distinct evaluation metrics.\n",
        "\n",
        "### 1. Lexical Coverage (Quantitative)\n",
        "\n",
        "This metric quantifies the **vocabulary coverage** of each model, measuring the percentage of unique tokens from our cleaned datasets that are found within the model's learned vocabulary.\n",
        "\n",
        "This is a critical test for comparing the two architectures. Word2Vec, being a word-level model, is inherently limited to its training vocabulary and cannot represent **out-of-vocabulary (OOV)** words. In contrast, FastText, which learns vectors for character n-grams (subwords), can construct vectors for *any* word, including neologisms, misspellings, or rare words not encountered during training.\n",
        "\n",
        "### 2. Semantic Similarity (Quantitative)\n",
        "\n",
        "A successful embedding model should capture meaningful **semantic relationships**, placing words with similar meanings close together in the vector space and words with opposite meanings far apart.\n",
        "\n",
        "To quantify this, we measure the average **cosine similarity** for two predefined sets of word pairs:\n",
        "* **Synonym Pairs** (e.g., `yax≈üƒ±`, `…ôla`): We expect a high similarity score (close to 1.0), indicating semantic proximity.\n",
        "* **Antonym Pairs** (e.g., `yax≈üƒ±`, `pis`): We expect a low or negative similarity score (close to -1.0 or 0.0), indicating semantic distance.\n",
        "\n",
        "A \"Separation Score\" (calculated as `Synonym Similarity - Antonym Similarity`) is then used to provide a single, robust measure of the model's ability to discriminate between semantic similarity and opposition. A higher separation score is better.\n",
        "\n",
        "### 3. Nearest Neighbors Analysis (Qualitative)\n",
        "\n",
        "Beyond quantitative scores, a **qualitative analysis** of the embedding space is performed by inspecting the **nearest neighbors** for a set of predefined seed words.\n",
        "\n",
        "By examining the top 5 most similar words for a given seed (e.g., `bahalƒ±` or `pis`), we can intuitively assess the quality of the learned representations. This helps us judge whether the model has learned logical contexts (e.g., are the neighbors of \"expensive\" other price-related words?) or if it has merely learned superficial co-occurrence patterns."
      ],
      "metadata": {
        "id": "pZeYLp8x8-ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import re\n",
        "\n",
        "w2v = Word2Vec.load(\"embeddings/word2vec.model\")\n",
        "ft  = FastText.load(\"embeddings/fasttext.model\")\n",
        "\n",
        "seed_words = [\"yax≈üƒ±\",\"pis\",\"√ßox\",\"bahalƒ±\",\"ucuz\",\"m√ºk…ômm…ôl\",\"d…ôh≈ü…ôt\",\"<PRICE>\",\"<RATING_POS>\"]\n",
        "syn_pairs  = [(\"yax≈üƒ±\",\"…ôla\"), (\"bahalƒ±\",\"qiym…ôtli\"), (\"ucuz\",\"s…ôrf…ôli\")]\n",
        "ant_pairs  = [(\"yax≈üƒ±\",\"pis\"), (\"bahalƒ±\",\"ucuz\")]\n",
        "\n",
        "def lexical_coverage(model, tokens):\n",
        "    vocab = model.wv.key_to_index\n",
        "    return sum(1 for t in tokens if t in vocab) / max(1,len(tokens))\n",
        "\n",
        "files = [\n",
        "    f\"./clean_data/labeled-sentiment_2col.xlsx\",\n",
        "    f\"./clean_data/test_1_2col.xlsx\",\n",
        "    f\"./clean_data/train_3_2col.xlsx\",\n",
        "    f\"./clean_data/train-00000-of-00001_2col.xlsx\",\n",
        "    f\"./clean_data/merged_dataset_CSV_1_2col.xlsx\",\n",
        "]\n",
        "\n",
        "def read_tokens(f):\n",
        "    df = pd.read_excel(f, usecols=[\"cleaned_text\"])\n",
        "    return [t for row in df[\"cleaned_text\"].astype(str) for t in row.split()]\n",
        "\n",
        "print(\"== Lexical coverage (per dataset) ==\")\n",
        "for f in files:\n",
        "    toks = read_tokens(f)\n",
        "    cov_w2v = lexical_coverage(w2v, toks)\n",
        "    cov_ftv = lexical_coverage(ft, toks)  # FT still embeds OOV via subwords\n",
        "    print(f\"{f}: W2V={cov_w2v:.3f}, FT(vocab)={cov_ftv:.3f}\")\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos(a,b): return float(dot(a,b)/(norm(a)*norm(b)))\n",
        "\n",
        "def pair_sim(model, pairs):\n",
        "    vals = []\n",
        "    for a,b in pairs:\n",
        "        try: vals.append(model.wv.similarity(a,b))\n",
        "        except KeyError: pass\n",
        "    return sum(vals)/len(vals) if vals else float('nan')\n",
        "\n",
        "syn_w2v = pair_sim(w2v, syn_pairs)\n",
        "syn_ft  = pair_sim(ft,  syn_pairs)\n",
        "ant_w2v = pair_sim(w2v, ant_pairs)\n",
        "ant_ft  = pair_sim(ft,  ant_pairs)\n",
        "\n",
        "print(\"\\n== Similarity (higher better for synonyms; lower better for antonyms) ==\")\n",
        "print(f\"Synonyms: W2V={syn_w2v:.3f}, FT={syn_ft:.3f}\")\n",
        "print(f\"Antonyms: W2V={ant_w2v:.3f}, FT={ant_ft:.3f}\")\n",
        "print(f\"Separation (Syn - Ant): W2V={(syn_w2v - ant_w2v):.3f}, FT={(syn_ft - ant_ft):.3f}\")\n",
        "\n",
        "def neighbors(model, word, k=5):\n",
        "  try: return [w for w,_ in model.wv.most_similar(word, topn=k)]\n",
        "  except KeyError: return []\n",
        "\n",
        "print(\"\\n== Nearest neighbors (qualitative) ==\")\n",
        "for w in seed_words:\n",
        "  print(f\"  W2V NN for '{w}':\", neighbors(w2v, w))\n",
        "  print(f\"  FT  NN for '{w}':\", neighbors(ft,  w))\n",
        "\n",
        "# (Optional) domain drift if you train domain-specific models separately:\n",
        "# drift(word, model_a, model_b) = 1 - cos(vec_a, vec_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKK5bBK080FU",
        "outputId": "96ea6d8c-486e-4724-d0cb-a7d852b8fed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Lexical coverage (per dataset) ==\n",
            "./clean_data/labeled-sentiment_2col.xlsx: W2V=0.930, FT(vocab)=0.930\n",
            "./clean_data/test_1_2col.xlsx: W2V=0.915, FT(vocab)=0.915\n",
            "./clean_data/train_3_2col.xlsx: W2V=0.919, FT(vocab)=0.919\n",
            "./clean_data/train-00000-of-00001_2col.xlsx: W2V=0.923, FT(vocab)=0.923\n",
            "./clean_data/merged_dataset_CSV_1_2col.xlsx: W2V=0.899, FT(vocab)=0.899\n",
            "\n",
            "== Similarity (higher better for synonyms; lower better for antonyms) ==\n",
            "Synonyms: W2V=0.361, FT=0.465\n",
            "Antonyms: W2V=0.335, FT=0.424\n",
            "Separation (Syn - Ant): W2V=0.027, FT=0.040\n",
            "\n",
            "== Nearest neighbors (qualitative) ==\n",
            "  W2V NN for 'yax≈üƒ±': ['<RATING_POS>', 'yaxshi', 'iyi', 'yax≈üƒ±.', 'olar.']\n",
            "  FT  NN for 'yax≈üƒ±': ['yax≈üƒ±!', 'yax≈üƒ±ƒ±', 'yax≈üƒ±.', 'yax≈üƒ±kƒ±', ',yax≈üƒ±']\n",
            "  W2V NN for 'pis': ['<RATING_NEG>', '<STARS_LOW>', 'pis.', 'pisdir', 'pisdi']\n",
            "  FT  NN for 'pis': ['pis!', 'pis,', 'pis.', 'pis.pul', 'piis']\n",
            "  W2V NN for '√ßox': ['√ßoox', '√ß√∂x', 't…ôtbiqidir', '…ôladir', 'elverisli']\n",
            "  FT  NN for '√ßox': ['√ßox√ßox', '√ßox.√ßox', '(√ßox', '\"√ßox', '-√ßox']\n",
            "  W2V NN for 'bahalƒ±': ['portretlerin…ô', 'villalarƒ±', 'yaxtalarƒ±', '≈ü…ôb…ôk…ôdi', 'metallarla']\n",
            "  FT  NN for 'bahalƒ±': ['bahalƒ±ƒ±', 'bahalƒ±sƒ±', 'bahalƒ±q', 'baharlƒ±', 'baha,']\n",
            "  W2V NN for 'ucuz': ['d√ºz…ôltdirilib', '≈üeytanbazardan', 'sududu', 'keyfiyetli', 'sorbasi']\n",
            "  FT  NN for 'ucuz': ['ucuz,', 'ucuz.', 'ucuza', 'ucuzu', 'ucuzdu']\n",
            "  W2V NN for 'm√ºk…ômm…ôl': ['s√ºjetli', 'm√∂ht…ô≈ü…ômm', 'm√ºk…ômm…ôll', 's√ºper', 'k…ôlim…ôyl…ô']\n",
            "  FT  NN for 'm√ºk…ômm…ôl': ['m√ºk…ômm…ôl!', 'm√ºk…ômm…ôl,', 'm√ºk…ômm…ôl.', 'm√ºk…ômm…ôll', 'm√ºk…ômm…ôldi.']\n",
            "  W2V NN for 'd…ôh≈ü…ôt': ['xal√ßalardan', 'ayranlarƒ±', 't…ôsirlidi.', 'soundtreki', 'a√ßdƒ±q.']\n",
            "  FT  NN for 'd…ôh≈ü…ôt': ['d…ôh≈ü…ôt!', 'd…ôh≈ü…ôtd√º', 'd…ôh≈ü…ôt…ô', 'd…ôh≈ü…ôti', 'd…ôh≈ü…ôtizm']\n",
            "  W2V NN for '<PRICE>': ['√ºnvanƒ±n', '√∂d…ônis', 'zakazƒ±', 'kardan', 'aldadƒ±rlar.']\n",
            "  FT  NN for '<PRICE>': ['<PRICE>a', '3m', '500mb', '5m', 'nfs']\n",
            "  W2V NN for '<RATING_POS>': ['b…ôy…ônilsin', \"…ô'la\", '√∂n…ôrir…ôm', 'g…ô≈ü…ông', 's√∂zl…ô.']\n",
            "  FT  NN for '<RATING_POS>': ['√ßooxx', '√ßox.√ßox', '√ßoox', '<RATING_NEG>', '√ßoxx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not quite sure about its correctness,\n",
        "# but as the vocab sizes differs significantly, it's possible to observe inconsistent values\n",
        "\n",
        "# Domain Drift Analysis\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import random\n",
        "\n",
        "def cos_sim(v1, v2):\n",
        "    return dot(v1, v2) / (norm(v1) * norm(v2))\n",
        "\n",
        "def domain_drift(word, model_a, model_b):\n",
        "    try:\n",
        "        vec_a = model_a.wv[word]\n",
        "        vec_b = model_b.wv[word]\n",
        "        return 1 - cos_sim(vec_a, vec_b)\n",
        "    except KeyError:\n",
        "        return None\n",
        "\n",
        "# Read the domain-tagged corpus\n",
        "domain_sentences = {\"news\": [], \"social\": [], \"reviews\": [], \"general\": []}\n",
        "\n",
        "with open(\"corpus_all.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line.startswith(\"dom\"):\n",
        "            parts = line.split(maxsplit=1)\n",
        "            if len(parts) == 2:\n",
        "                domain_tag = parts[0].replace(\"dom\", \"\")\n",
        "                text = parts[1]\n",
        "                if domain_tag in domain_sentences:\n",
        "                    domain_sentences[domain_tag].append(text.split())\n",
        "\n",
        "# Filter the domans based on the sentences numbers\n",
        "min_samples = 2000\n",
        "valid_domains = {k: v for k, v in domain_sentences.items() if len(v) >= min_samples}\n",
        "\n",
        "print(f\"=== Domain Statistics ===\")\n",
        "for dom, sents in domain_sentences.items():\n",
        "    print(f\"{dom:10s}: {len(sents):6d} sentences\")\n",
        "\n",
        "if len(valid_domains) < 2:\n",
        "    print(f\"\\nNeed at least 2 domains with ‚â•{min_samples} sentences for comparison.\")\n",
        "else:\n",
        "    # Find the smallest domain\n",
        "    sample_size = min(len(sents) for sents in valid_domains.values())\n",
        "    print(f\"\\n=== Balancing: using {sample_size} sentences per domain ===\")\n",
        "\n",
        "    # take equal numbered samples from each domain\n",
        "    #random.seed(42)\n",
        "    balanced_domains = {}\n",
        "    for dom, sents in valid_domains.items():\n",
        "        balanced_domains[dom] = random.sample(sents, sample_size)\n",
        "\n",
        "    # train the models\n",
        "    domain_models = {}\n",
        "    print(\"\\n=== Training balanced Word2Vec models ===\")\n",
        "    for dom, sents in balanced_domains.items():\n",
        "        model = Word2Vec(sentences=sents, vector_size=200, window=5,\n",
        "                        min_count=3, sg=1, epochs=10, seed=42)\n",
        "        domain_models[dom] = model\n",
        "        print(f\"Trained {dom}: {len(sents)} sentences, vocab={len(model.wv)}\")\n",
        "\n",
        "    # Domain drift analysis\n",
        "    test_words = [\"yax≈üƒ±\", \"pis\", \"√ßox\", \"yax≈üƒ±dƒ±r\", \"pisdir\", \"m√ºk…ômm…ôl\",\n",
        "                  \"telefon\", \"film\", \"m…ôhsul\"]\n",
        "\n",
        "    print(\"\\n=== Domain Drift Analysis (Balanced Data) ===\")\n",
        "    print(\"Lower = similar context, Higher = different context\\n\")\n",
        "\n",
        "    domains = list(domain_models.keys())\n",
        "    for i, dom_a in enumerate(domains):\n",
        "        for dom_b in domains[i+1:]:\n",
        "            print(f\"\\n{dom_a.upper()} ‚Üî {dom_b.upper()}:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            drifts = []\n",
        "            for w in test_words:\n",
        "                drift = domain_drift(w, domain_models[dom_a], domain_models[dom_b])\n",
        "                if drift is not None:\n",
        "                    drifts.append((w, drift))\n",
        "\n",
        "            if drifts:\n",
        "                drifts.sort(key=lambda x: x[1])\n",
        "                for w, d in drifts:\n",
        "                    status = \"‚úì stable\" if d < 0.3 else \"‚ö† drift\" if d < 0.6 else \"‚úó different\"\n",
        "                    print(f\"  {w:15s} ‚Üí {d:.3f}  {status}\")\n",
        "            else:\n",
        "                print(\"  (no common words)\")"
      ],
      "metadata": {
        "id": "Wmi46jh6ZSNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}