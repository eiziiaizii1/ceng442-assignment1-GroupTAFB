{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENG442 Assignment 1 - Azerbaijani Text Preprocessing & Word Embeddings\n",
    "\n",
    "**Group Members:**\n",
    "* Talha Ubeydullah Gamga | 20050111078\n",
    "* Aziz Önder | 22050141021\n",
    "* Muhammed Fatih Asan | 23050151026\n",
    "* Buğra Bildiren | 20050111022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "In this step, we import all necessary libraries for data processing and text cleaning, including standard libraries like `pandas`, `re` (regex), and `ftfy` (for text normalization).\n",
    "\n",
    "We also import the custom utility functions (e.g., domain detection, emoji/negation handling) from the `ozel_temizlik.py` script.\n",
    "\n",
    "Finally, we define and create the `OUTPUT_DIR` (`clean_data/`) where our processed Excel files will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Utility functions from 'ozel_temizlik.py' imported.\n",
      "Output directory 'clean_data' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import ftfy\n",
    "\n",
    "# --- Import Custom Utility Script ---\n",
    "# This script contains helper functions for domain detection,\n",
    "# negation, emoji mapping, and other specific cleaning tasks.\n",
    "import ozel_temizlik\n",
    "\n",
    "# --- Setup Output Directory ---\n",
    "OUTPUT_DIR = \"clean_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"Utility functions from 'ozel_temizlik.py' imported.\")\n",
    "print(f\"Output directory '{OUTPUT_DIR}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Core Helper Functions\n",
    "\n",
    "In this step, we define the core helper functions required by the main processing pipeline. These functions are responsible for:\n",
    "\n",
    "1.  **`map_sentiment_value`**: Standardizing the various sentiment labels (e.g., \"positive\", 1, 0.0) from the 5 datasets into a single numeric float format (0.0, 0.5, 1.0).\n",
    "2.  **`lower_az`**: Handling the specific lowercase conversion for Azerbaijani characters (e.g., 'İ' -> 'i', 'I' -> 'ı').\n",
    "3.  **`basic_regex_clean`**: Applying the fundamental, non-domain-specific cleaning rules (like removing HTML, normalizing URLs, Emails, Numbers) based on the code snippets provided in the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core helper functions (map_sentiment_value, lower_az, basic_regex_clean) defined.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# 2.1: Sentiment Label Standardization\n",
    "# (Maps all labels to 0.0, 0.5, 1.0 as float)\n",
    "# ----------------------------------------------------------------\n",
    "def map_sentiment_value(label):\n",
    "    \"\"\"\n",
    "    Converts various sentiment labels (str, int) from different\n",
    "    datasets into a standard float value (0.0, 0.5, or 1.0).\n",
    "    Returns None if the label is unmappable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle string labels\n",
    "    if isinstance(label, str):\n",
    "        label_low = label.lower().strip()\n",
    "        if label_low in ['positive', 'pos', '1']:\n",
    "            return 1.0\n",
    "        elif label_low in ['negative', 'neg', '0']:\n",
    "            return 0.0\n",
    "        elif label_low in ['neutral', 'neu', '0.5']:\n",
    "            return 0.5\n",
    "    \n",
    "    # Handle integer labels\n",
    "    if isinstance(label, int):\n",
    "        if label == 1:\n",
    "            return 1.0\n",
    "        elif label == 0:\n",
    "            return 0.0\n",
    "            \n",
    "    # Handle float labels\n",
    "    try:\n",
    "        f_label = float(label)\n",
    "        if f_label == 1.0: return 1.0\n",
    "        if f_label == 0.0: return 0.0\n",
    "        if f_label == 0.5: return 0.5\n",
    "    except (ValueError, TypeError):\n",
    "        pass \n",
    "    \n",
    "    # If no match is found\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2.2: Azerbaijani-Specific Lowercasing\n",
    "# (PDF Section 5.1.4: 'İ' -> 'i', 'I' -> 'ı')\n",
    "# ----------------------------------------------------------------\n",
    "def lower_az(text):\n",
    "    \"\"\"Applies Azerbaijani-specific lowercase conversion.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) # Ensure input is string\n",
    "    text = text.replace('İ', 'i').replace('I', 'ı')\n",
    "    return text.lower() # Apply standard lowercasing\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2.3: Basic Text Normalization (Regex)\n",
    "# (Based on PDF Section 5.1 code snippets)\n",
    "# ----------------------------------------------------------------\n",
    "def basic_regex_clean(text):\n",
    "    \"\"\"\n",
    "    Applies fundamental regex cleaning rules as specified \n",
    "    in the assignment PDF (e.g., HTML, URL, EMAIL, NUM).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fix broken Unicode (e.g., â€™ -> ’) - Recommended by PDF\n",
    "    text = ftfy.fix_text(text)\n",
    "    \n",
    "    # Remove HTML tags (PDF Section 5.1.1)\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Normalize URLs (PDF Section 5.1.2)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '<URL>', text)\n",
    "    \n",
    "    # Normalize Emails (PDF Section 5.1.2)\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    \n",
    "    # Normalize @mentions (PDF Section 5.1.2)\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    \n",
    "    # Normalize Phone (simple rule) (PDF Section 5.1.2)\n",
    "    # (Note: PDF has a typo r'(\\+?d... , corrected to \\d)\n",
    "    text = re.sub(r'(\\+?\\d[\\d\\s-]{7,}\\d)', '<PHONE>', text)\n",
    "    \n",
    "    # Normalize Numbers (as per PDF Section 5.1.6)\n",
    "    text = re.sub(r'\\b\\d+[\\.,\\d]*\\b', '<NUM>', text)\n",
    "    \n",
    "    # Normalize repeating characters (e.g., çooox -> çoxx) (PDF Section 5.1.6)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Core helper functions (map_sentiment_value, lower_az, basic_regex_clean) defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Main Normalization Pipeline (normalize_text_az)\n",
    "\n",
    "This is the main \"glue\" function for our pipeline. It's responsible for executing all cleaning steps in the correct logical order.\n",
    "\n",
    "It combines the **basic** cleaning functions (defined in Step 2, e.g., `basic_regex_clean`, `lower_az`) with the **advanced/specialized** functions imported from `ozel_temizlik.py` (e.g., `split_hashtags`, `handle_negation`).\n",
    "\n",
    "The main `process_file` function (which we will use in the next step) will call this single function to perform the complete text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOP5ho6E9SVvdzyJbpekIxs",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
